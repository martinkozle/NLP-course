{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratory exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "import string\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tqdm\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from scripts.word_embeddings import load_embedding_weights\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Input, Embedding\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "from scripts.nlg_evaluation import score_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = 'data/OpinosisDataset1.0_0/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/OpinosisDataset1.0_0/topics\\\\accuracy_garmin_nuvi_255W_gps.txt.data',\n",
       " 'data/OpinosisDataset1.0_0/topics\\\\bathroom_bestwestern_hotel_sfo.txt.data',\n",
       " 'data/OpinosisDataset1.0_0/topics\\\\battery-life_amazon_kindle.txt.data',\n",
       " 'data/OpinosisDataset1.0_0/topics\\\\battery-life_ipod_nano_8gb.txt.data',\n",
       " 'data/OpinosisDataset1.0_0/topics\\\\battery-life_netbook_1005ha.txt.data']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_file_paths = glob.glob(BASE_PATH + 'topics/*.txt.data')\n",
    "topic_file_paths[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/OpinosisDataset1.0_0/summaries-gold\\\\accuracy_garmin_nuvi_255W_gps\\\\accuracy_garmin_nuvi_255W_gps.1.gold',\n",
       " 'data/OpinosisDataset1.0_0/summaries-gold\\\\accuracy_garmin_nuvi_255W_gps\\\\accuracy_garmin_nuvi_255W_gps.2.gold',\n",
       " 'data/OpinosisDataset1.0_0/summaries-gold\\\\accuracy_garmin_nuvi_255W_gps\\\\accuracy_garmin_nuvi_255W_gps.3.gold',\n",
       " 'data/OpinosisDataset1.0_0/summaries-gold\\\\accuracy_garmin_nuvi_255W_gps\\\\accuracy_garmin_nuvi_255W_gps.4.gold',\n",
       " 'data/OpinosisDataset1.0_0/summaries-gold\\\\accuracy_garmin_nuvi_255W_gps\\\\accuracy_garmin_nuvi_255W_gps.5.gold']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries_file_paths = glob.glob(BASE_PATH + 'summaries-gold/*/*.gold')\n",
    "summaries_file_paths[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_name</th>\n",
       "      <th>topic_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>accuracy_garmin_nuvi_255W_gps</td>\n",
       "      <td>, and is very, very accurate .\\n but for the m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bathroom_bestwestern_hotel_sfo</td>\n",
       "      <td>The room was not overly big, but clean and ve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>battery-life_amazon_kindle</td>\n",
       "      <td>After I plugged it in to my USB hub on my com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>battery-life_ipod_nano_8gb</td>\n",
       "      <td>short battery life  I moved up from an 8gb .\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>battery-life_netbook_1005ha</td>\n",
       "      <td>6GHz 533FSB cpu, glossy display, 3, Cell 23Wh ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       topic_name  \\\n",
       "0   accuracy_garmin_nuvi_255W_gps   \n",
       "1  bathroom_bestwestern_hotel_sfo   \n",
       "2      battery-life_amazon_kindle   \n",
       "3      battery-life_ipod_nano_8gb   \n",
       "4     battery-life_netbook_1005ha   \n",
       "\n",
       "                                          topic_text  \n",
       "0  , and is very, very accurate .\\n but for the m...  \n",
       "1   The room was not overly big, but clean and ve...  \n",
       "2   After I plugged it in to my USB hub on my com...  \n",
       "3   short battery life  I moved up from an 8gb .\\...  \n",
       "4  6GHz 533FSB cpu, glossy display, 3, Cell 23Wh ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "for topic_file_path in topic_file_paths:\n",
    "    with open(topic_file_path, 'r') as f:\n",
    "        base_name = os.path.basename(f.name)\n",
    "        topic_name = re.match(r'(.*)\\.txt\\.data', base_name).group(1)\n",
    "        topic_text = f.read()\n",
    "        data.append(\n",
    "            {\n",
    "                'topic_name': topic_name,\n",
    "                'topic_text': topic_text\n",
    "            }\n",
    "        )\n",
    "df_topic = pd.DataFrame(data)\n",
    "df_topic.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_name</th>\n",
       "      <th>summary_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>accuracy_garmin_nuvi_255W_gps</td>\n",
       "      <td>This unit is generally quite accurate.  \\nSet-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>accuracy_garmin_nuvi_255W_gps</td>\n",
       "      <td>The Garmin seems to be generally very accurate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>accuracy_garmin_nuvi_255W_gps</td>\n",
       "      <td>It is very accurate, even in destination time.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>accuracy_garmin_nuvi_255W_gps</td>\n",
       "      <td>Very accurate with travel and destination time...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>accuracy_garmin_nuvi_255W_gps</td>\n",
       "      <td>Its accurate, fast and its simple operations m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      topic_name  \\\n",
       "0  accuracy_garmin_nuvi_255W_gps   \n",
       "1  accuracy_garmin_nuvi_255W_gps   \n",
       "2  accuracy_garmin_nuvi_255W_gps   \n",
       "3  accuracy_garmin_nuvi_255W_gps   \n",
       "4  accuracy_garmin_nuvi_255W_gps   \n",
       "\n",
       "                                        summary_text  \n",
       "0  This unit is generally quite accurate.  \\nSet-...  \n",
       "1  The Garmin seems to be generally very accurate...  \n",
       "2   It is very accurate, even in destination time.\\n  \n",
       "3  Very accurate with travel and destination time...  \n",
       "4  Its accurate, fast and its simple operations m...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "for summary_file_path in summaries_file_paths:\n",
    "    with open(summary_file_path, 'r') as f:\n",
    "        base_name = os.path.basename(f.name)\n",
    "        topic_name = re.match(r'(.*)\\.\\d\\.gold', base_name).group(1)\n",
    "        summary_text = f.read()\n",
    "        data.append(\n",
    "            {\n",
    "                'topic_name': topic_name,\n",
    "                'summary_text': summary_text\n",
    "            }\n",
    "        )\n",
    "df_summary = pd.DataFrame(data)\n",
    "df_summary.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_name</th>\n",
       "      <th>topic_text</th>\n",
       "      <th>summary_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>accuracy_garmin_nuvi_255W_gps</td>\n",
       "      <td>, and is very, very accurate .\\n but for the m...</td>\n",
       "      <td>This unit is generally quite accurate.  \\nSet-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>accuracy_garmin_nuvi_255W_gps</td>\n",
       "      <td>, and is very, very accurate .\\n but for the m...</td>\n",
       "      <td>The Garmin seems to be generally very accurate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>accuracy_garmin_nuvi_255W_gps</td>\n",
       "      <td>, and is very, very accurate .\\n but for the m...</td>\n",
       "      <td>It is very accurate, even in destination time.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>accuracy_garmin_nuvi_255W_gps</td>\n",
       "      <td>, and is very, very accurate .\\n but for the m...</td>\n",
       "      <td>Very accurate with travel and destination time...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>accuracy_garmin_nuvi_255W_gps</td>\n",
       "      <td>, and is very, very accurate .\\n but for the m...</td>\n",
       "      <td>Its accurate, fast and its simple operations m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>voice_garmin_nuvi_255W_gps</td>\n",
       "      <td>The voice prompts and maps are wonderful esp...</td>\n",
       "      <td>The voice is a bit robotic.\\nThe voice is very...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>voice_garmin_nuvi_255W_gps</td>\n",
       "      <td>The voice prompts and maps are wonderful esp...</td>\n",
       "      <td>The voices sound robotic.\\nTTS mode is the mos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>voice_garmin_nuvi_255W_gps</td>\n",
       "      <td>The voice prompts and maps are wonderful esp...</td>\n",
       "      <td>255W garmin gps has more than 750 voices but t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>voice_garmin_nuvi_255W_gps</td>\n",
       "      <td>The voice prompts and maps are wonderful esp...</td>\n",
       "      <td>Voice is clear and sweet.\\nVoice commands are ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>voice_garmin_nuvi_255W_gps</td>\n",
       "      <td>The voice prompts and maps are wonderful esp...</td>\n",
       "      <td>The voice is very clear and loud.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>238 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        topic_name  \\\n",
       "0    accuracy_garmin_nuvi_255W_gps   \n",
       "1    accuracy_garmin_nuvi_255W_gps   \n",
       "2    accuracy_garmin_nuvi_255W_gps   \n",
       "3    accuracy_garmin_nuvi_255W_gps   \n",
       "4    accuracy_garmin_nuvi_255W_gps   \n",
       "..                             ...   \n",
       "233     voice_garmin_nuvi_255W_gps   \n",
       "234     voice_garmin_nuvi_255W_gps   \n",
       "235     voice_garmin_nuvi_255W_gps   \n",
       "236     voice_garmin_nuvi_255W_gps   \n",
       "237     voice_garmin_nuvi_255W_gps   \n",
       "\n",
       "                                            topic_text  \\\n",
       "0    , and is very, very accurate .\\n but for the m...   \n",
       "1    , and is very, very accurate .\\n but for the m...   \n",
       "2    , and is very, very accurate .\\n but for the m...   \n",
       "3    , and is very, very accurate .\\n but for the m...   \n",
       "4    , and is very, very accurate .\\n but for the m...   \n",
       "..                                                 ...   \n",
       "233    The voice prompts and maps are wonderful esp...   \n",
       "234    The voice prompts and maps are wonderful esp...   \n",
       "235    The voice prompts and maps are wonderful esp...   \n",
       "236    The voice prompts and maps are wonderful esp...   \n",
       "237    The voice prompts and maps are wonderful esp...   \n",
       "\n",
       "                                          summary_text  \n",
       "0    This unit is generally quite accurate.  \\nSet-...  \n",
       "1    The Garmin seems to be generally very accurate...  \n",
       "2     It is very accurate, even in destination time.\\n  \n",
       "3    Very accurate with travel and destination time...  \n",
       "4    Its accurate, fast and its simple operations m...  \n",
       "..                                                 ...  \n",
       "233  The voice is a bit robotic.\\nThe voice is very...  \n",
       "234  The voices sound robotic.\\nTTS mode is the mos...  \n",
       "235  255W garmin gps has more than 750 voices but t...  \n",
       "236  Voice is clear and sweet.\\nVoice commands are ...  \n",
       "237                  The voice is very clear and loud.  \n",
       "\n",
       "[238 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw = df_topic.merge(df_summary, on='topic_name')\n",
    "df_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_name</th>\n",
       "      <th>topic_text</th>\n",
       "      <th>summary_text</th>\n",
       "      <th>topic_tokenized</th>\n",
       "      <th>summary_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>accuracy_garmin_nuvi_255W_gps</td>\n",
       "      <td>, and is very, very accurate .\\n but for the m...</td>\n",
       "      <td>This unit is generally quite accurate.  \\nSet-...</td>\n",
       "      <td>[,, and, is, very, ,, very, accurate, ., but, ...</td>\n",
       "      <td>[this, unit, is, generally, quite, accurate, ....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>accuracy_garmin_nuvi_255W_gps</td>\n",
       "      <td>, and is very, very accurate .\\n but for the m...</td>\n",
       "      <td>The Garmin seems to be generally very accurate...</td>\n",
       "      <td>[,, and, is, very, ,, very, accurate, ., but, ...</td>\n",
       "      <td>[the, garmin, seems, to, be, generally, very, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>accuracy_garmin_nuvi_255W_gps</td>\n",
       "      <td>, and is very, very accurate .\\n but for the m...</td>\n",
       "      <td>It is very accurate, even in destination time.\\n</td>\n",
       "      <td>[,, and, is, very, ,, very, accurate, ., but, ...</td>\n",
       "      <td>[it, is, very, accurate, ,, even, in, destinat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>accuracy_garmin_nuvi_255W_gps</td>\n",
       "      <td>, and is very, very accurate .\\n but for the m...</td>\n",
       "      <td>Very accurate with travel and destination time...</td>\n",
       "      <td>[,, and, is, very, ,, very, accurate, ., but, ...</td>\n",
       "      <td>[very, accurate, with, travel, and, destinatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>accuracy_garmin_nuvi_255W_gps</td>\n",
       "      <td>, and is very, very accurate .\\n but for the m...</td>\n",
       "      <td>Its accurate, fast and its simple operations m...</td>\n",
       "      <td>[,, and, is, very, ,, very, accurate, ., but, ...</td>\n",
       "      <td>[its, accurate, ,, fast, and, its, simple, ope...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>voice_garmin_nuvi_255W_gps</td>\n",
       "      <td>The voice prompts and maps are wonderful esp...</td>\n",
       "      <td>The voice is a bit robotic.\\nThe voice is very...</td>\n",
       "      <td>[the, voice, prompts, and, maps, are, wonderfu...</td>\n",
       "      <td>[the, voice, is, a, bit, robotic, ., the, voic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>voice_garmin_nuvi_255W_gps</td>\n",
       "      <td>The voice prompts and maps are wonderful esp...</td>\n",
       "      <td>The voices sound robotic.\\nTTS mode is the mos...</td>\n",
       "      <td>[the, voice, prompts, and, maps, are, wonderfu...</td>\n",
       "      <td>[the, voices, sound, robotic, ., tts, mode, is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>voice_garmin_nuvi_255W_gps</td>\n",
       "      <td>The voice prompts and maps are wonderful esp...</td>\n",
       "      <td>255W garmin gps has more than 750 voices but t...</td>\n",
       "      <td>[the, voice, prompts, and, maps, are, wonderfu...</td>\n",
       "      <td>[255w, garmin, gps, has, more, than, 750, voic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>voice_garmin_nuvi_255W_gps</td>\n",
       "      <td>The voice prompts and maps are wonderful esp...</td>\n",
       "      <td>Voice is clear and sweet.\\nVoice commands are ...</td>\n",
       "      <td>[the, voice, prompts, and, maps, are, wonderfu...</td>\n",
       "      <td>[voice, is, clear, and, sweet, ., voice, comma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>voice_garmin_nuvi_255W_gps</td>\n",
       "      <td>The voice prompts and maps are wonderful esp...</td>\n",
       "      <td>The voice is very clear and loud.</td>\n",
       "      <td>[the, voice, prompts, and, maps, are, wonderfu...</td>\n",
       "      <td>[the, voice, is, very, clear, and, loud, .]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>238 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        topic_name  \\\n",
       "0    accuracy_garmin_nuvi_255W_gps   \n",
       "1    accuracy_garmin_nuvi_255W_gps   \n",
       "2    accuracy_garmin_nuvi_255W_gps   \n",
       "3    accuracy_garmin_nuvi_255W_gps   \n",
       "4    accuracy_garmin_nuvi_255W_gps   \n",
       "..                             ...   \n",
       "233     voice_garmin_nuvi_255W_gps   \n",
       "234     voice_garmin_nuvi_255W_gps   \n",
       "235     voice_garmin_nuvi_255W_gps   \n",
       "236     voice_garmin_nuvi_255W_gps   \n",
       "237     voice_garmin_nuvi_255W_gps   \n",
       "\n",
       "                                            topic_text  \\\n",
       "0    , and is very, very accurate .\\n but for the m...   \n",
       "1    , and is very, very accurate .\\n but for the m...   \n",
       "2    , and is very, very accurate .\\n but for the m...   \n",
       "3    , and is very, very accurate .\\n but for the m...   \n",
       "4    , and is very, very accurate .\\n but for the m...   \n",
       "..                                                 ...   \n",
       "233    The voice prompts and maps are wonderful esp...   \n",
       "234    The voice prompts and maps are wonderful esp...   \n",
       "235    The voice prompts and maps are wonderful esp...   \n",
       "236    The voice prompts and maps are wonderful esp...   \n",
       "237    The voice prompts and maps are wonderful esp...   \n",
       "\n",
       "                                          summary_text  \\\n",
       "0    This unit is generally quite accurate.  \\nSet-...   \n",
       "1    The Garmin seems to be generally very accurate...   \n",
       "2     It is very accurate, even in destination time.\\n   \n",
       "3    Very accurate with travel and destination time...   \n",
       "4    Its accurate, fast and its simple operations m...   \n",
       "..                                                 ...   \n",
       "233  The voice is a bit robotic.\\nThe voice is very...   \n",
       "234  The voices sound robotic.\\nTTS mode is the mos...   \n",
       "235  255W garmin gps has more than 750 voices but t...   \n",
       "236  Voice is clear and sweet.\\nVoice commands are ...   \n",
       "237                  The voice is very clear and loud.   \n",
       "\n",
       "                                       topic_tokenized  \\\n",
       "0    [,, and, is, very, ,, very, accurate, ., but, ...   \n",
       "1    [,, and, is, very, ,, very, accurate, ., but, ...   \n",
       "2    [,, and, is, very, ,, very, accurate, ., but, ...   \n",
       "3    [,, and, is, very, ,, very, accurate, ., but, ...   \n",
       "4    [,, and, is, very, ,, very, accurate, ., but, ...   \n",
       "..                                                 ...   \n",
       "233  [the, voice, prompts, and, maps, are, wonderfu...   \n",
       "234  [the, voice, prompts, and, maps, are, wonderfu...   \n",
       "235  [the, voice, prompts, and, maps, are, wonderfu...   \n",
       "236  [the, voice, prompts, and, maps, are, wonderfu...   \n",
       "237  [the, voice, prompts, and, maps, are, wonderfu...   \n",
       "\n",
       "                                     summary_tokenized  \n",
       "0    [this, unit, is, generally, quite, accurate, ....  \n",
       "1    [the, garmin, seems, to, be, generally, very, ...  \n",
       "2    [it, is, very, accurate, ,, even, in, destinat...  \n",
       "3    [very, accurate, with, travel, and, destinatio...  \n",
       "4    [its, accurate, ,, fast, and, its, simple, ope...  \n",
       "..                                                 ...  \n",
       "233  [the, voice, is, a, bit, robotic, ., the, voic...  \n",
       "234  [the, voices, sound, robotic, ., tts, mode, is...  \n",
       "235  [255w, garmin, gps, has, more, than, 750, voic...  \n",
       "236  [voice, is, clear, and, sweet, ., voice, comma...  \n",
       "237        [the, voice, is, very, clear, and, loud, .]  \n",
       "\n",
       "[238 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw['topic_tokenized'] = df_raw['topic_text'].apply(lambda x: word_tokenize(x.lower()))\n",
    "df_raw['summary_tokenized'] = df_raw['summary_text'].apply(lambda x: word_tokenize(x.lower()))\n",
    "df_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_stopwords_and_punctuation(tokens_list: pd.Series) -> pd.Series:\n",
    "    stopset = set(stopwords.words('english'))\n",
    "    punctuation = set(string.punctuation)\n",
    "    filter_set = stopset | punctuation\n",
    "\n",
    "    return tokens_list.apply(\n",
    "        lambda tokens:\n",
    "            np.array(list(filter(lambda token: token not in filter_set, tokens)))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw['topic_tokenized_filtered'] = filter_stopwords_and_punctuation(df_raw['topic_tokenized'])\n",
    "df_raw['summary_tokenized_filtered'] = filter_stopwords_and_punctuation(df_raw['summary_tokenized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw['summary_tokenized_filtered'] = df_raw['summary_tokenized_filtered'].apply(lambda x: ['<START>', *x, '<END>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [<START>, unit, generally, quite, accurate, se...\n",
       "1      [<START>, garmin, seems, generally, accurate, ...\n",
       "2      [<START>, accurate, even, destination, time, <...\n",
       "3      [<START>, accurate, travel, destination, time,...\n",
       "4      [<START>, accurate, fast, simple, operations, ...\n",
       "                             ...                        \n",
       "233    [<START>, voice, bit, robotic, voice, clear, l...\n",
       "234    [<START>, voices, sound, robotic, tts, mode, p...\n",
       "235    [<START>, 255w, garmin, gps, 750, voices, soun...\n",
       "236    [<START>, voice, clear, sweet, voice, commands...\n",
       "237                 [<START>, voice, clear, loud, <END>]\n",
       "Name: summary_tokenized_filtered, Length: 238, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw['summary_tokenized_filtered']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocabulary(tokens_list: pd.Series) -> pd.Series:\n",
    "    return tokens_list.explode().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "room            4214\n",
       "great           4185\n",
       "location        4183\n",
       "hotel           3409\n",
       "staff           3400\n",
       "                ... \n",
       "webupdates         3\n",
       "registration       3\n",
       "nt                 3\n",
       "webupdater         3\n",
       "atlas              3\n",
       "Name: topic_tokenized_filtered, Length: 7184, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = get_vocabulary(df_raw['topic_tokenized_filtered'])\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='topic_tokenized_filtered', ylabel='Count'>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "_, ax = plt.subplots(figsize=(15, 10))\n",
    "sns.histplot(vocabulary, bins=len(vocabulary.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocabulary_with_removed_less_than_k(vocabulary, k):\n",
    "    return vocabulary[vocabulary > k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "smaller_voc = vocabulary_with_removed_less_than_k(vocabulary, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "room        4214\n",
       "great       4185\n",
       "location    4183\n",
       "hotel       3409\n",
       "staff       3400\n",
       "            ... \n",
       "peak          11\n",
       "<START>        0\n",
       "<END>          0\n",
       "<OTHER>        0\n",
       "<PAD>          0\n",
       "Length: 2937, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smaller_voc = smaller_voc.append(pd.Series([0, 0, 0, 0], index=['<START>', '<END>', '<OTHER>', '<PAD>']))\n",
    "smaller_voc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Count'>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "_, ax = plt.subplots(figsize=(15, 10))\n",
    "sns.histplot(smaller_voc, bins=len(smaller_voc.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(smaller_voc)=2937\n"
     ]
    }
   ],
   "source": [
    "print(f'{len(smaller_voc)=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_val, x_test, y_train_val, y_test = train_test_split(\n",
    "    df_raw['topic_tokenized_filtered'], df_raw['summary_tokenized_filtered'], test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_id = {word: idx for idx, word in enumerate(smaller_voc.index)}\n",
    "id_to_word = {idx: word for idx, word in enumerate(smaller_voc.index)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for topic_tokens, summary_tokens in zip(x_train_val, y_train_val):\n",
    "    encoder_inputs = [\n",
    "        word_to_id.get(token, word_to_id['<OTHER>']) for token in topic_tokens\n",
    "    ]\n",
    "    accumulator = []\n",
    "    for summary_token in summary_tokens:\n",
    "        if accumulator:\n",
    "            new_row = {\n",
    "                'encoder_inputs': np.array(encoder_inputs),\n",
    "                'decoder_inputs': np.array(accumulator),\n",
    "                'decoder_outputs': summary_token\n",
    "            }\n",
    "            data.append(new_row)\n",
    "        accumulator.append(\n",
    "            word_to_id.get(summary_token, word_to_id['<OTHER>'])\n",
    "        )\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>encoder_inputs</th>\n",
       "      <th>decoder_inputs</th>\n",
       "      <th>decoder_outputs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[390, 1930, 2935, 297, 7, 2935, 180, 0, 138, 1...</td>\n",
       "      <td>[2933]</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[390, 1930, 2935, 297, 7, 2935, 180, 0, 138, 1...</td>\n",
       "      <td>[2933, 8]</td>\n",
       "      <td>service</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[390, 1930, 2935, 297, 7, 2935, 180, 0, 138, 1...</td>\n",
       "      <td>[2933, 8, 7]</td>\n",
       "      <td>sometimes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[390, 1930, 2935, 297, 7, 2935, 180, 0, 138, 1...</td>\n",
       "      <td>[2933, 8, 7, 412]</td>\n",
       "      <td>slow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[390, 1930, 2935, 297, 7, 2935, 180, 0, 138, 1...</td>\n",
       "      <td>[2933, 8, 7, 412, 348]</td>\n",
       "      <td>friendly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2031</th>\n",
       "      <td>[1, 2, 76, 2935, 1198, 645, 925, 336, 30, 516,...</td>\n",
       "      <td>[2933, 1182]</td>\n",
       "      <td>excellent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2032</th>\n",
       "      <td>[1, 2, 76, 2935, 1198, 645, 925, 336, 30, 516,...</td>\n",
       "      <td>[2933, 1182, 35]</td>\n",
       "      <td>nice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2033</th>\n",
       "      <td>[1, 2, 76, 2935, 1198, 645, 925, 336, 30, 516,...</td>\n",
       "      <td>[2933, 1182, 35, 13]</td>\n",
       "      <td>close</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2034</th>\n",
       "      <td>[1, 2, 76, 2935, 1198, 645, 925, 336, 30, 516,...</td>\n",
       "      <td>[2933, 1182, 35, 13, 125]</td>\n",
       "      <td>want</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2035</th>\n",
       "      <td>[1, 2, 76, 2935, 1198, 645, 925, 336, 30, 516,...</td>\n",
       "      <td>[2933, 1182, 35, 13, 125, 132]</td>\n",
       "      <td>&lt;END&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2036 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         encoder_inputs  \\\n",
       "0     [390, 1930, 2935, 297, 7, 2935, 180, 0, 138, 1...   \n",
       "1     [390, 1930, 2935, 297, 7, 2935, 180, 0, 138, 1...   \n",
       "2     [390, 1930, 2935, 297, 7, 2935, 180, 0, 138, 1...   \n",
       "3     [390, 1930, 2935, 297, 7, 2935, 180, 0, 138, 1...   \n",
       "4     [390, 1930, 2935, 297, 7, 2935, 180, 0, 138, 1...   \n",
       "...                                                 ...   \n",
       "2031  [1, 2, 76, 2935, 1198, 645, 925, 336, 30, 516,...   \n",
       "2032  [1, 2, 76, 2935, 1198, 645, 925, 336, 30, 516,...   \n",
       "2033  [1, 2, 76, 2935, 1198, 645, 925, 336, 30, 516,...   \n",
       "2034  [1, 2, 76, 2935, 1198, 645, 925, 336, 30, 516,...   \n",
       "2035  [1, 2, 76, 2935, 1198, 645, 925, 336, 30, 516,...   \n",
       "\n",
       "                      decoder_inputs decoder_outputs  \n",
       "0                             [2933]            good  \n",
       "1                          [2933, 8]         service  \n",
       "2                       [2933, 8, 7]       sometimes  \n",
       "3                  [2933, 8, 7, 412]            slow  \n",
       "4             [2933, 8, 7, 412, 348]        friendly  \n",
       "...                              ...             ...  \n",
       "2031                    [2933, 1182]       excellent  \n",
       "2032                [2933, 1182, 35]            nice  \n",
       "2033            [2933, 1182, 35, 13]           close  \n",
       "2034       [2933, 1182, 35, 13, 125]            want  \n",
       "2035  [2933, 1182, 35, 13, 125, 132]           <END>  \n",
       "\n",
       "[2036 rows x 3 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_binarizer = LabelBinarizer()\n",
    "label_binarizer.fit(smaller_voc.index)\n",
    "decoder_outputs_one_hot = label_binarizer.transform(df['decoder_outputs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\martinkozle\\anaconda3\\envs\\py38_ml\\lib\\site-packages\\pandas\\core\\frame.py:3678: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self[col] = igetitem(value, i)\n"
     ]
    }
   ],
   "source": [
    "columns = ['decoder_outputs_' + word for word in smaller_voc.index]\n",
    "df[columns] = decoder_outputs_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>encoder_inputs</th>\n",
       "      <th>decoder_inputs</th>\n",
       "      <th>decoder_outputs</th>\n",
       "      <th>decoder_outputs_room</th>\n",
       "      <th>decoder_outputs_great</th>\n",
       "      <th>decoder_outputs_location</th>\n",
       "      <th>decoder_outputs_hotel</th>\n",
       "      <th>decoder_outputs_staff</th>\n",
       "      <th>decoder_outputs_rooms</th>\n",
       "      <th>decoder_outputs_battery</th>\n",
       "      <th>...</th>\n",
       "      <th>decoder_outputs_platinum</th>\n",
       "      <th>decoder_outputs_firefox</th>\n",
       "      <th>decoder_outputs_refurbishment</th>\n",
       "      <th>decoder_outputs_regardless</th>\n",
       "      <th>decoder_outputs_efficiency</th>\n",
       "      <th>decoder_outputs_peak</th>\n",
       "      <th>decoder_outputs_&lt;START&gt;</th>\n",
       "      <th>decoder_outputs_&lt;END&gt;</th>\n",
       "      <th>decoder_outputs_&lt;OTHER&gt;</th>\n",
       "      <th>decoder_outputs_&lt;PAD&gt;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[390, 1930, 2935, 297, 7, 2935, 180, 0, 138, 1...</td>\n",
       "      <td>[2933]</td>\n",
       "      <td>good</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[390, 1930, 2935, 297, 7, 2935, 180, 0, 138, 1...</td>\n",
       "      <td>[2933, 8]</td>\n",
       "      <td>service</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[390, 1930, 2935, 297, 7, 2935, 180, 0, 138, 1...</td>\n",
       "      <td>[2933, 8, 7]</td>\n",
       "      <td>sometimes</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[390, 1930, 2935, 297, 7, 2935, 180, 0, 138, 1...</td>\n",
       "      <td>[2933, 8, 7, 412]</td>\n",
       "      <td>slow</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[390, 1930, 2935, 297, 7, 2935, 180, 0, 138, 1...</td>\n",
       "      <td>[2933, 8, 7, 412, 348]</td>\n",
       "      <td>friendly</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2031</th>\n",
       "      <td>[1, 2, 76, 2935, 1198, 645, 925, 336, 30, 516,...</td>\n",
       "      <td>[2933, 1182]</td>\n",
       "      <td>excellent</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2032</th>\n",
       "      <td>[1, 2, 76, 2935, 1198, 645, 925, 336, 30, 516,...</td>\n",
       "      <td>[2933, 1182, 35]</td>\n",
       "      <td>nice</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2033</th>\n",
       "      <td>[1, 2, 76, 2935, 1198, 645, 925, 336, 30, 516,...</td>\n",
       "      <td>[2933, 1182, 35, 13]</td>\n",
       "      <td>close</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2034</th>\n",
       "      <td>[1, 2, 76, 2935, 1198, 645, 925, 336, 30, 516,...</td>\n",
       "      <td>[2933, 1182, 35, 13, 125]</td>\n",
       "      <td>want</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2035</th>\n",
       "      <td>[1, 2, 76, 2935, 1198, 645, 925, 336, 30, 516,...</td>\n",
       "      <td>[2933, 1182, 35, 13, 125, 132]</td>\n",
       "      <td>&lt;END&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2036 rows × 2940 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         encoder_inputs  \\\n",
       "0     [390, 1930, 2935, 297, 7, 2935, 180, 0, 138, 1...   \n",
       "1     [390, 1930, 2935, 297, 7, 2935, 180, 0, 138, 1...   \n",
       "2     [390, 1930, 2935, 297, 7, 2935, 180, 0, 138, 1...   \n",
       "3     [390, 1930, 2935, 297, 7, 2935, 180, 0, 138, 1...   \n",
       "4     [390, 1930, 2935, 297, 7, 2935, 180, 0, 138, 1...   \n",
       "...                                                 ...   \n",
       "2031  [1, 2, 76, 2935, 1198, 645, 925, 336, 30, 516,...   \n",
       "2032  [1, 2, 76, 2935, 1198, 645, 925, 336, 30, 516,...   \n",
       "2033  [1, 2, 76, 2935, 1198, 645, 925, 336, 30, 516,...   \n",
       "2034  [1, 2, 76, 2935, 1198, 645, 925, 336, 30, 516,...   \n",
       "2035  [1, 2, 76, 2935, 1198, 645, 925, 336, 30, 516,...   \n",
       "\n",
       "                      decoder_inputs decoder_outputs  decoder_outputs_room  \\\n",
       "0                             [2933]            good                     0   \n",
       "1                          [2933, 8]         service                     0   \n",
       "2                       [2933, 8, 7]       sometimes                     0   \n",
       "3                  [2933, 8, 7, 412]            slow                     0   \n",
       "4             [2933, 8, 7, 412, 348]        friendly                     0   \n",
       "...                              ...             ...                   ...   \n",
       "2031                    [2933, 1182]       excellent                     0   \n",
       "2032                [2933, 1182, 35]            nice                     0   \n",
       "2033            [2933, 1182, 35, 13]           close                     0   \n",
       "2034       [2933, 1182, 35, 13, 125]            want                     0   \n",
       "2035  [2933, 1182, 35, 13, 125, 132]           <END>                     0   \n",
       "\n",
       "      decoder_outputs_great  decoder_outputs_location  decoder_outputs_hotel  \\\n",
       "0                         0                         0                      0   \n",
       "1                         0                         0                      0   \n",
       "2                         0                         0                      0   \n",
       "3                         0                         0                      0   \n",
       "4                         0                         0                      0   \n",
       "...                     ...                       ...                    ...   \n",
       "2031                      0                         0                      0   \n",
       "2032                      0                         0                      0   \n",
       "2033                      0                         0                      0   \n",
       "2034                      0                         0                      0   \n",
       "2035                      0                         0                      0   \n",
       "\n",
       "      decoder_outputs_staff  decoder_outputs_rooms  decoder_outputs_battery  \\\n",
       "0                         0                      0                        0   \n",
       "1                         0                      0                        0   \n",
       "2                         0                      0                        0   \n",
       "3                         0                      0                        0   \n",
       "4                         0                      0                        0   \n",
       "...                     ...                    ...                      ...   \n",
       "2031                      0                      0                        0   \n",
       "2032                      0                      0                        0   \n",
       "2033                      0                      0                        0   \n",
       "2034                      0                      0                        0   \n",
       "2035                      0                      0                        0   \n",
       "\n",
       "      ...  decoder_outputs_platinum  decoder_outputs_firefox  \\\n",
       "0     ...                         0                        0   \n",
       "1     ...                         0                        0   \n",
       "2     ...                         0                        0   \n",
       "3     ...                         0                        0   \n",
       "4     ...                         0                        0   \n",
       "...   ...                       ...                      ...   \n",
       "2031  ...                         0                        0   \n",
       "2032  ...                         0                        0   \n",
       "2033  ...                         0                        0   \n",
       "2034  ...                         0                        0   \n",
       "2035  ...                         0                        0   \n",
       "\n",
       "      decoder_outputs_refurbishment  decoder_outputs_regardless  \\\n",
       "0                                 0                           0   \n",
       "1                                 0                           0   \n",
       "2                                 0                           0   \n",
       "3                                 0                           0   \n",
       "4                                 0                           0   \n",
       "...                             ...                         ...   \n",
       "2031                              0                           0   \n",
       "2032                              0                           0   \n",
       "2033                              0                           0   \n",
       "2034                              0                           0   \n",
       "2035                              0                           0   \n",
       "\n",
       "      decoder_outputs_efficiency  decoder_outputs_peak  \\\n",
       "0                              0                     0   \n",
       "1                              0                     0   \n",
       "2                              0                     0   \n",
       "3                              0                     0   \n",
       "4                              0                     0   \n",
       "...                          ...                   ...   \n",
       "2031                           0                     0   \n",
       "2032                           0                     0   \n",
       "2033                           0                     0   \n",
       "2034                           0                     0   \n",
       "2035                           0                     0   \n",
       "\n",
       "      decoder_outputs_<START>  decoder_outputs_<END>  decoder_outputs_<OTHER>  \\\n",
       "0                           0                      0                        0   \n",
       "1                           0                      0                        0   \n",
       "2                           0                      0                        0   \n",
       "3                           0                      0                        0   \n",
       "4                           0                      0                        0   \n",
       "...                       ...                    ...                      ...   \n",
       "2031                        0                      0                        0   \n",
       "2032                        0                      0                        0   \n",
       "2033                        0                      0                        0   \n",
       "2034                        0                      0                        0   \n",
       "2035                        0                      0                        0   \n",
       "\n",
       "      decoder_outputs_<PAD>  \n",
       "0                         0  \n",
       "1                         0  \n",
       "2                         0  \n",
       "3                         0  \n",
       "4                         0  \n",
       "...                     ...  \n",
       "2031                      0  \n",
       "2032                      0  \n",
       "2033                      0  \n",
       "2034                      0  \n",
       "2035                      0  \n",
       "\n",
       "[2036 rows x 2940 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(padding_size_encoder, padding_size_decoder, vocabulary_size, embedding_size=None, embeddings=None):\n",
    "    encoder_inputs = Input(shape=(padding_size_encoder,))\n",
    "    if embeddings is not None:\n",
    "        encoder_embedding = Embedding(input_dim=vocabulary_size, output_dim=embedding_size,\n",
    "                                      weights=[embeddings], trainable=False)(encoder_inputs)\n",
    "    else:\n",
    "        encoder_embedding = Embedding(input_dim=vocabulary_size, output_dim=embedding_size, trainable=True)(encoder_inputs)\n",
    "    encoder = LSTM(128, return_state=True)\n",
    "    _, state_h, state_c = encoder(encoder_embedding)\n",
    "    encoder_states = [state_h, state_c]\n",
    "\n",
    "    decoder_inputs = Input(shape=(padding_size_decoder,))\n",
    "    if embeddings is not None:\n",
    "        decoder_embedding = Embedding(input_dim=vocabulary_size, output_dim=embedding_size,\n",
    "                                    weights=[embeddings], trainable=False)(decoder_inputs)\n",
    "    else:\n",
    "        decoder_embedding = Embedding(input_dim=vocabulary_size, output_dim=embedding_size,\n",
    "                                    trainable=True)(decoder_inputs)\n",
    "    decoder = LSTM(128, return_state=True)\n",
    "    decoder_outputs, _, _ = decoder(\n",
    "        decoder_embedding, initial_state=encoder_states)\n",
    "\n",
    "    decoder_outputs = Dense(\n",
    "        vocabulary_size, activation='softmax')(decoder_outputs)\n",
    "\n",
    "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=0.01),\n",
    "                  loss=categorical_crossentropy)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1368"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padding_size_encoder = int(df['encoder_inputs'].apply(len).mean())\n",
    "padding_size_encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I do the padding at the end of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MARTIN~1\\AppData\\Local\\Temp/ipykernel_10980/4189259931.py:1: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['encoder_inputs_padded'] = list(pad_sequences(df['encoder_inputs'], padding='post', maxlen=padding_size_encoder, value=word_to_id['<PAD>']))\n"
     ]
    }
   ],
   "source": [
    "df['encoder_inputs_padded'] = list(pad_sequences(df['encoder_inputs'], padding='post', maxlen=padding_size_encoder, value=word_to_id['<PAD>']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['encoder_inputs_padded'].apply(len).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding_size_decoder = df_raw['summary_tokenized'].apply(len).max() + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MARTIN~1\\AppData\\Local\\Temp/ipykernel_10980/1196234322.py:1: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['decoder_inputs_padded'] = list(pad_sequences(df['decoder_inputs'], maxlen=padding_size_decoder, value=word_to_id['<PAD>']))\n"
     ]
    }
   ],
   "source": [
    "df['decoder_inputs_padded'] = list(pad_sequences(df['decoder_inputs'], maxlen=padding_size_decoder, value=word_to_id['<PAD>']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_no_embeddings = create_model(padding_size_encoder, padding_size_decoder, len(smaller_voc), 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>encoder_inputs</th>\n",
       "      <th>decoder_inputs</th>\n",
       "      <th>decoder_outputs</th>\n",
       "      <th>decoder_outputs_room</th>\n",
       "      <th>decoder_outputs_great</th>\n",
       "      <th>decoder_outputs_location</th>\n",
       "      <th>decoder_outputs_hotel</th>\n",
       "      <th>decoder_outputs_staff</th>\n",
       "      <th>decoder_outputs_rooms</th>\n",
       "      <th>decoder_outputs_battery</th>\n",
       "      <th>...</th>\n",
       "      <th>decoder_outputs_refurbishment</th>\n",
       "      <th>decoder_outputs_regardless</th>\n",
       "      <th>decoder_outputs_efficiency</th>\n",
       "      <th>decoder_outputs_peak</th>\n",
       "      <th>decoder_outputs_&lt;START&gt;</th>\n",
       "      <th>decoder_outputs_&lt;END&gt;</th>\n",
       "      <th>decoder_outputs_&lt;OTHER&gt;</th>\n",
       "      <th>decoder_outputs_&lt;PAD&gt;</th>\n",
       "      <th>encoder_inputs_padded</th>\n",
       "      <th>decoder_inputs_padded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[390, 1930, 2935, 297, 7, 2935, 180, 0, 138, 1...</td>\n",
       "      <td>[2933]</td>\n",
       "      <td>good</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[7, 227, 2, 1, 131, 7, 22, 334, 58, 2, 1, 390,...</td>\n",
       "      <td>[2936, 2936, 2936, 2936, 2936, 2936, 2936, 293...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[390, 1930, 2935, 297, 7, 2935, 180, 0, 138, 1...</td>\n",
       "      <td>[2933, 8]</td>\n",
       "      <td>service</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[7, 227, 2, 1, 131, 7, 22, 334, 58, 2, 1, 390,...</td>\n",
       "      <td>[2936, 2936, 2936, 2936, 2936, 2936, 2936, 293...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[390, 1930, 2935, 297, 7, 2935, 180, 0, 138, 1...</td>\n",
       "      <td>[2933, 8, 7]</td>\n",
       "      <td>sometimes</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[7, 227, 2, 1, 131, 7, 22, 334, 58, 2, 1, 390,...</td>\n",
       "      <td>[2936, 2936, 2936, 2936, 2936, 2936, 2936, 293...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[390, 1930, 2935, 297, 7, 2935, 180, 0, 138, 1...</td>\n",
       "      <td>[2933, 8, 7, 412]</td>\n",
       "      <td>slow</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[7, 227, 2, 1, 131, 7, 22, 334, 58, 2, 1, 390,...</td>\n",
       "      <td>[2936, 2936, 2936, 2936, 2936, 2936, 2936, 293...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[390, 1930, 2935, 297, 7, 2935, 180, 0, 138, 1...</td>\n",
       "      <td>[2933, 8, 7, 412, 348]</td>\n",
       "      <td>friendly</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[7, 227, 2, 1, 131, 7, 22, 334, 58, 2, 1, 390,...</td>\n",
       "      <td>[2936, 2936, 2936, 2936, 2936, 2936, 2936, 293...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2942 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      encoder_inputs          decoder_inputs  \\\n",
       "0  [390, 1930, 2935, 297, 7, 2935, 180, 0, 138, 1...                  [2933]   \n",
       "1  [390, 1930, 2935, 297, 7, 2935, 180, 0, 138, 1...               [2933, 8]   \n",
       "2  [390, 1930, 2935, 297, 7, 2935, 180, 0, 138, 1...            [2933, 8, 7]   \n",
       "3  [390, 1930, 2935, 297, 7, 2935, 180, 0, 138, 1...       [2933, 8, 7, 412]   \n",
       "4  [390, 1930, 2935, 297, 7, 2935, 180, 0, 138, 1...  [2933, 8, 7, 412, 348]   \n",
       "\n",
       "  decoder_outputs  decoder_outputs_room  decoder_outputs_great  \\\n",
       "0            good                     0                      0   \n",
       "1         service                     0                      0   \n",
       "2       sometimes                     0                      0   \n",
       "3            slow                     0                      0   \n",
       "4        friendly                     0                      0   \n",
       "\n",
       "   decoder_outputs_location  decoder_outputs_hotel  decoder_outputs_staff  \\\n",
       "0                         0                      0                      0   \n",
       "1                         0                      0                      0   \n",
       "2                         0                      0                      0   \n",
       "3                         0                      0                      0   \n",
       "4                         0                      0                      0   \n",
       "\n",
       "   decoder_outputs_rooms  decoder_outputs_battery  ...  \\\n",
       "0                      0                        0  ...   \n",
       "1                      0                        0  ...   \n",
       "2                      0                        0  ...   \n",
       "3                      0                        0  ...   \n",
       "4                      0                        0  ...   \n",
       "\n",
       "   decoder_outputs_refurbishment  decoder_outputs_regardless  \\\n",
       "0                              0                           0   \n",
       "1                              0                           0   \n",
       "2                              0                           0   \n",
       "3                              0                           0   \n",
       "4                              0                           0   \n",
       "\n",
       "   decoder_outputs_efficiency  decoder_outputs_peak  decoder_outputs_<START>  \\\n",
       "0                           0                     0                        0   \n",
       "1                           0                     0                        0   \n",
       "2                           0                     0                        0   \n",
       "3                           0                     0                        0   \n",
       "4                           0                     0                        0   \n",
       "\n",
       "   decoder_outputs_<END>  decoder_outputs_<OTHER>  decoder_outputs_<PAD>  \\\n",
       "0                      0                        0                      0   \n",
       "1                      0                        0                      0   \n",
       "2                      0                        0                      0   \n",
       "3                      0                        0                      0   \n",
       "4                      0                        0                      0   \n",
       "\n",
       "                               encoder_inputs_padded  \\\n",
       "0  [7, 227, 2, 1, 131, 7, 22, 334, 58, 2, 1, 390,...   \n",
       "1  [7, 227, 2, 1, 131, 7, 22, 334, 58, 2, 1, 390,...   \n",
       "2  [7, 227, 2, 1, 131, 7, 22, 334, 58, 2, 1, 390,...   \n",
       "3  [7, 227, 2, 1, 131, 7, 22, 334, 58, 2, 1, 390,...   \n",
       "4  [7, 227, 2, 1, 131, 7, 22, 334, 58, 2, 1, 390,...   \n",
       "\n",
       "                               decoder_inputs_padded  \n",
       "0  [2936, 2936, 2936, 2936, 2936, 2936, 2936, 293...  \n",
       "1  [2936, 2936, 2936, 2936, 2936, 2936, 2936, 293...  \n",
       "2  [2936, 2936, 2936, 2936, 2936, 2936, 2936, 293...  \n",
       "3  [2936, 2936, 2936, 2936, 2936, 2936, 2936, 293...  \n",
       "4  [2936, 2936, 2936, 2936, 2936, 2936, 2936, 293...  \n",
       "\n",
       "[5 rows x 2942 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_columns = ['encoder_inputs', 'decoder_inputs',\n",
    "                'decoder_outputs', 'encoder_inputs_padded', 'decoder_inputs_padded']\n",
    "(\n",
    "    X_train, X_val, Y_train, Y_val\n",
    ") = train_test_split(df[['encoder_inputs_padded', 'decoder_inputs_padded']],\n",
    "                     df.drop(drop_columns, axis=1),\n",
    "                     test_size=1/9,\n",
    "                     random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_list = [np.stack(X_train['encoder_inputs_padded']), np.stack(X_train['decoder_inputs_padded'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_list = [np.stack(X_val['encoder_inputs_padded']), np.stack(X_val['decoder_inputs_padded'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "29/29 [==============================] - 9s 149ms/step - loss: 6.4675 - val_loss: 6.2237\n",
      "Epoch 2/200\n",
      "29/29 [==============================] - 3s 119ms/step - loss: 5.3767 - val_loss: 6.2969\n",
      "Epoch 3/200\n",
      "29/29 [==============================] - 3s 118ms/step - loss: 5.1888 - val_loss: 6.4264\n",
      "Epoch 4/200\n",
      "29/29 [==============================] - 3s 118ms/step - loss: 4.9910 - val_loss: 6.5422\n",
      "Epoch 5/200\n",
      "29/29 [==============================] - 3s 117ms/step - loss: 4.7162 - val_loss: 6.7030\n",
      "Epoch 6/200\n",
      "29/29 [==============================] - 4s 129ms/step - loss: 4.3620 - val_loss: 6.9378\n",
      "Epoch 7/200\n",
      "29/29 [==============================] - 4s 130ms/step - loss: 3.9874 - val_loss: 7.0346\n",
      "Epoch 8/200\n",
      "29/29 [==============================] - 4s 124ms/step - loss: 3.5795 - val_loss: 7.2343\n",
      "Epoch 9/200\n",
      "29/29 [==============================] - 4s 124ms/step - loss: 3.9234 - val_loss: 6.9980\n",
      "Epoch 10/200\n",
      "29/29 [==============================] - 3s 117ms/step - loss: 2.9371 - val_loss: 7.1026\n",
      "Epoch 11/200\n",
      "29/29 [==============================] - 4s 123ms/step - loss: 2.5447 - val_loss: 7.2573\n",
      "Epoch 12/200\n",
      "29/29 [==============================] - 3s 121ms/step - loss: 2.1935 - val_loss: 7.3841\n",
      "Epoch 13/200\n",
      "29/29 [==============================] - 3s 118ms/step - loss: nan - val_loss: nan\n",
      "Epoch 14/200\n",
      "29/29 [==============================] - 3s 117ms/step - loss: nan - val_loss: nan\n",
      "Epoch 15/200\n",
      "29/29 [==============================] - 3s 118ms/step - loss: nan - val_loss: nan\n",
      "Epoch 16/200\n",
      "29/29 [==============================] - 3s 118ms/step - loss: nan - val_loss: nan\n",
      "Epoch 17/200\n",
      "29/29 [==============================] - 3s 117ms/step - loss: nan - val_loss: nan\n",
      "Epoch 18/200\n",
      "29/29 [==============================] - 3s 118ms/step - loss: nan - val_loss: nan\n",
      "Epoch 19/200\n",
      "29/29 [==============================] - 3s 116ms/step - loss: nan - val_loss: nan\n",
      "Epoch 20/200\n",
      "29/29 [==============================] - 4s 125ms/step - loss: nan - val_loss: nan\n",
      "Epoch 21/200\n",
      "29/29 [==============================] - 4s 129ms/step - loss: nan - val_loss: nan\n",
      "Epoch 22/200\n",
      "29/29 [==============================] - 4s 143ms/step - loss: nan - val_loss: nan\n",
      "Epoch 23/200\n",
      "29/29 [==============================] - 4s 136ms/step - loss: nan - val_loss: nan\n",
      "Epoch 24/200\n",
      "29/29 [==============================] - 4s 125ms/step - loss: nan - val_loss: nan\n",
      "Epoch 25/200\n",
      "29/29 [==============================] - 3s 118ms/step - loss: nan - val_loss: nan\n",
      "Epoch 26/200\n",
      "29/29 [==============================] - 4s 131ms/step - loss: nan - val_loss: nan\n",
      "Epoch 27/200\n",
      "29/29 [==============================] - 4s 126ms/step - loss: nan - val_loss: nan\n",
      "Epoch 28/200\n",
      "29/29 [==============================] - 4s 124ms/step - loss: nan - val_loss: nan\n",
      "Epoch 29/200\n",
      "29/29 [==============================] - 4s 126ms/step - loss: nan - val_loss: nan\n",
      "Epoch 30/200\n",
      "29/29 [==============================] - 4s 124ms/step - loss: nan - val_loss: nan\n",
      "Epoch 31/200\n",
      "29/29 [==============================] - 4s 121ms/step - loss: nan - val_loss: nan\n",
      "Epoch 32/200\n",
      "29/29 [==============================] - 4s 124ms/step - loss: nan - val_loss: nan\n",
      "Epoch 33/200\n",
      "29/29 [==============================] - 3s 118ms/step - loss: nan - val_loss: nan\n",
      "Epoch 34/200\n",
      "29/29 [==============================] - 4s 125ms/step - loss: nan - val_loss: nan\n",
      "Epoch 35/200\n",
      "29/29 [==============================] - 4s 122ms/step - loss: nan - val_loss: nan\n",
      "Epoch 36/200\n",
      "29/29 [==============================] - 3s 120ms/step - loss: nan - val_loss: nan\n",
      "Epoch 37/200\n",
      "29/29 [==============================] - 3s 117ms/step - loss: nan - val_loss: nan\n",
      "Epoch 38/200\n",
      "29/29 [==============================] - 4s 122ms/step - loss: nan - val_loss: nan\n",
      "Epoch 39/200\n",
      "29/29 [==============================] - 3s 117ms/step - loss: nan - val_loss: nan\n",
      "Epoch 40/200\n",
      "29/29 [==============================] - 4s 123ms/step - loss: nan - val_loss: nan\n",
      "Epoch 41/200\n",
      "29/29 [==============================] - 4s 125ms/step - loss: nan - val_loss: nan\n",
      "Epoch 42/200\n",
      "29/29 [==============================] - 4s 123ms/step - loss: nan - val_loss: nan\n",
      "Epoch 43/200\n",
      "29/29 [==============================] - 3s 118ms/step - loss: nan - val_loss: nan\n",
      "Epoch 44/200\n",
      "29/29 [==============================] - 3s 117ms/step - loss: nan - val_loss: nan\n",
      "Epoch 45/200\n",
      "29/29 [==============================] - 3s 117ms/step - loss: nan - val_loss: nan\n",
      "Epoch 46/200\n",
      "29/29 [==============================] - 3s 117ms/step - loss: nan - val_loss: nan\n",
      "Epoch 47/200\n",
      "29/29 [==============================] - 3s 117ms/step - loss: nan - val_loss: nan\n",
      "Epoch 48/200\n",
      "29/29 [==============================] - 3s 117ms/step - loss: nan - val_loss: nan\n",
      "Epoch 49/200\n",
      "29/29 [==============================] - 3s 117ms/step - loss: nan - val_loss: nan\n",
      "Epoch 50/200\n",
      "29/29 [==============================] - 3s 117ms/step - loss: nan - val_loss: nan\n",
      "Epoch 51/200\n",
      "29/29 [==============================] - 3s 117ms/step - loss: nan - val_loss: nan\n",
      "Epoch 52/200\n",
      "29/29 [==============================] - 3s 117ms/step - loss: nan - val_loss: nan\n",
      "Epoch 53/200\n",
      "29/29 [==============================] - 3s 117ms/step - loss: nan - val_loss: nan\n",
      "Epoch 54/200\n",
      "29/29 [==============================] - 4s 125ms/step - loss: nan - val_loss: nan\n",
      "Epoch 55/200\n",
      "29/29 [==============================] - 4s 124ms/step - loss: nan - val_loss: nan\n",
      "Epoch 56/200\n",
      "29/29 [==============================] - 3s 119ms/step - loss: nan - val_loss: nan\n",
      "Epoch 57/200\n",
      "29/29 [==============================] - 3s 118ms/step - loss: nan - val_loss: nan\n",
      "Epoch 58/200\n",
      "29/29 [==============================] - 3s 118ms/step - loss: nan - val_loss: nan\n",
      "Epoch 59/200\n",
      "29/29 [==============================] - 3s 117ms/step - loss: nan - val_loss: nan\n",
      "Epoch 60/200\n",
      "29/29 [==============================] - 3s 117ms/step - loss: nan - val_loss: nan\n",
      "Epoch 61/200\n",
      "29/29 [==============================] - 3s 117ms/step - loss: nan - val_loss: nan\n",
      "Epoch 62/200\n",
      "29/29 [==============================] - 3s 118ms/step - loss: nan - val_loss: nan\n",
      "Epoch 63/200\n",
      "29/29 [==============================] - 3s 117ms/step - loss: nan - val_loss: nan\n",
      "Epoch 64/200\n",
      "29/29 [==============================] - 3s 118ms/step - loss: nan - val_loss: nan\n",
      "Epoch 65/200\n",
      "29/29 [==============================] - 3s 118ms/step - loss: nan - val_loss: nan\n",
      "Epoch 66/200\n",
      "29/29 [==============================] - 3s 117ms/step - loss: nan - val_loss: nan\n",
      "Epoch 67/200\n",
      "29/29 [==============================] - 3s 117ms/step - loss: nan - val_loss: nan\n",
      "Epoch 68/200\n",
      "29/29 [==============================] - 3s 117ms/step - loss: nan - val_loss: nan\n",
      "Epoch 69/200\n",
      "29/29 [==============================] - 3s 117ms/step - loss: nan - val_loss: nan\n",
      "Epoch 70/200\n",
      "29/29 [==============================] - 3s 117ms/step - loss: nan - val_loss: nan\n",
      "Epoch 71/200\n",
      "29/29 [==============================] - 3s 117ms/step - loss: nan - val_loss: nan\n",
      "Epoch 72/200\n",
      "29/29 [==============================] - 3s 117ms/step - loss: nan - val_loss: nan\n",
      "Epoch 73/200\n",
      "29/29 [==============================] - 3s 117ms/step - loss: nan - val_loss: nan\n",
      "Epoch 74/200\n",
      "29/29 [==============================] - 3s 117ms/step - loss: nan - val_loss: nan\n",
      "Epoch 75/200\n",
      "29/29 [==============================] - 3s 118ms/step - loss: nan - val_loss: nan\n",
      "Epoch 76/200\n",
      "29/29 [==============================] - 4s 135ms/step - loss: nan - val_loss: nan\n",
      "Epoch 77/200\n",
      "29/29 [==============================] - 4s 133ms/step - loss: nan - val_loss: nan\n",
      "Epoch 78/200\n",
      "29/29 [==============================] - 4s 135ms/step - loss: nan - val_loss: nan\n",
      "Epoch 79/200\n",
      "29/29 [==============================] - 4s 138ms/step - loss: nan - val_loss: nan\n",
      "Epoch 80/200\n",
      "29/29 [==============================] - 4s 125ms/step - loss: nan - val_loss: nan\n",
      "Epoch 81/200\n",
      "29/29 [==============================] - 4s 129ms/step - loss: nan - val_loss: nan\n",
      "Epoch 82/200\n",
      "29/29 [==============================] - 4s 143ms/step - loss: nan - val_loss: nan\n",
      "Epoch 83/200\n",
      "29/29 [==============================] - 4s 121ms/step - loss: nan - val_loss: nan\n",
      "Epoch 84/200\n",
      "29/29 [==============================] - 4s 123ms/step - loss: nan - val_loss: nan\n",
      "Epoch 85/200\n",
      "29/29 [==============================] - 4s 134ms/step - loss: nan - val_loss: nan\n",
      "Epoch 86/200\n",
      "29/29 [==============================] - 4s 148ms/step - loss: nan - val_loss: nan\n",
      "Epoch 87/200\n",
      "29/29 [==============================] - 4s 137ms/step - loss: nan - val_loss: nan\n",
      "Epoch 88/200\n",
      "29/29 [==============================] - 4s 145ms/step - loss: nan - val_loss: nan\n",
      "Epoch 89/200\n",
      "29/29 [==============================] - 4s 143ms/step - loss: nan - val_loss: nan\n",
      "Epoch 90/200\n",
      "29/29 [==============================] - 4s 134ms/step - loss: nan - val_loss: nan\n",
      "Epoch 91/200\n",
      "29/29 [==============================] - 4s 137ms/step - loss: nan - val_loss: nan\n",
      "Epoch 92/200\n",
      "29/29 [==============================] - 4s 142ms/step - loss: nan - val_loss: nan\n",
      "Epoch 93/200\n",
      "29/29 [==============================] - 4s 145ms/step - loss: nan - val_loss: nan\n",
      "Epoch 94/200\n",
      "29/29 [==============================] - 4s 146ms/step - loss: nan - val_loss: nan\n",
      "Epoch 95/200\n",
      "29/29 [==============================] - 4s 141ms/step - loss: nan - val_loss: nan\n",
      "Epoch 96/200\n",
      "29/29 [==============================] - 4s 142ms/step - loss: nan - val_loss: nan\n",
      "Epoch 97/200\n",
      "29/29 [==============================] - 4s 136ms/step - loss: nan - val_loss: nan\n",
      "Epoch 98/200\n",
      "29/29 [==============================] - 4s 140ms/step - loss: nan - val_loss: nan\n",
      "Epoch 99/200\n",
      "29/29 [==============================] - 4s 142ms/step - loss: nan - val_loss: nan\n",
      "Epoch 100/200\n",
      "29/29 [==============================] - 4s 128ms/step - loss: nan - val_loss: nan\n",
      "Epoch 101/200\n",
      "29/29 [==============================] - 4s 127ms/step - loss: nan - val_loss: nan\n",
      "Epoch 102/200\n",
      "29/29 [==============================] - 4s 128ms/step - loss: nan - val_loss: nan\n",
      "Epoch 103/200\n",
      "29/29 [==============================] - 4s 134ms/step - loss: nan - val_loss: nan\n",
      "Epoch 104/200\n",
      "29/29 [==============================] - 4s 137ms/step - loss: nan - val_loss: nan\n",
      "Epoch 105/200\n",
      "29/29 [==============================] - 4s 153ms/step - loss: nan - val_loss: nan\n",
      "Epoch 106/200\n",
      "29/29 [==============================] - 4s 143ms/step - loss: nan - val_loss: nan\n",
      "Epoch 107/200\n",
      "29/29 [==============================] - 4s 155ms/step - loss: nan - val_loss: nan\n",
      "Epoch 108/200\n",
      "29/29 [==============================] - 4s 142ms/step - loss: nan - val_loss: nan\n",
      "Epoch 109/200\n",
      "29/29 [==============================] - 4s 134ms/step - loss: nan - val_loss: nan\n",
      "Epoch 110/200\n",
      "29/29 [==============================] - 4s 122ms/step - loss: nan - val_loss: nan\n",
      "Epoch 111/200\n",
      "29/29 [==============================] - 3s 119ms/step - loss: nan - val_loss: nan\n",
      "Epoch 112/200\n",
      "29/29 [==============================] - 3s 118ms/step - loss: nan - val_loss: nan\n",
      "Epoch 113/200\n",
      "29/29 [==============================] - 4s 135ms/step - loss: nan - val_loss: nan\n",
      "Epoch 114/200\n",
      "29/29 [==============================] - 3s 119ms/step - loss: nan - val_loss: nan\n",
      "Epoch 115/200\n",
      "29/29 [==============================] - 3s 119ms/step - loss: nan - val_loss: nan\n",
      "Epoch 116/200\n",
      "29/29 [==============================] - 3s 118ms/step - loss: nan - val_loss: nan\n",
      "Epoch 117/200\n",
      "29/29 [==============================] - 3s 118ms/step - loss: nan - val_loss: nan\n",
      "Epoch 118/200\n",
      "29/29 [==============================] - 3s 119ms/step - loss: nan - val_loss: nan\n",
      "Epoch 119/200\n",
      "29/29 [==============================] - 4s 134ms/step - loss: nan - val_loss: nan\n",
      "Epoch 120/200\n",
      "29/29 [==============================] - 4s 141ms/step - loss: nan - val_loss: nan\n",
      "Epoch 121/200\n",
      "29/29 [==============================] - 4s 148ms/step - loss: nan - val_loss: nan\n",
      "Epoch 122/200\n",
      "29/29 [==============================] - 4s 132ms/step - loss: nan - val_loss: nan\n",
      "Epoch 123/200\n",
      "29/29 [==============================] - 4s 130ms/step - loss: nan - val_loss: nan\n",
      "Epoch 124/200\n",
      "29/29 [==============================] - 4s 143ms/step - loss: nan - val_loss: nan\n",
      "Epoch 125/200\n",
      "29/29 [==============================] - 4s 131ms/step - loss: nan - val_loss: nan\n",
      "Epoch 126/200\n",
      "29/29 [==============================] - 4s 138ms/step - loss: nan - val_loss: nan\n",
      "Epoch 127/200\n",
      "29/29 [==============================] - 4s 144ms/step - loss: nan - val_loss: nan\n",
      "Epoch 128/200\n",
      "29/29 [==============================] - 4s 143ms/step - loss: nan - val_loss: nan\n",
      "Epoch 129/200\n",
      "29/29 [==============================] - 4s 131ms/step - loss: nan - val_loss: nan\n",
      "Epoch 130/200\n",
      "29/29 [==============================] - 4s 129ms/step - loss: nan - val_loss: nan\n",
      "Epoch 131/200\n",
      "29/29 [==============================] - 4s 129ms/step - loss: nan - val_loss: nan\n",
      "Epoch 132/200\n",
      "29/29 [==============================] - 4s 130ms/step - loss: nan - val_loss: nan\n",
      "Epoch 133/200\n",
      "29/29 [==============================] - 4s 129ms/step - loss: nan - val_loss: nan\n",
      "Epoch 134/200\n",
      "29/29 [==============================] - 4s 130ms/step - loss: nan - val_loss: nan\n",
      "Epoch 135/200\n",
      "29/29 [==============================] - 4s 146ms/step - loss: nan - val_loss: nan\n",
      "Epoch 136/200\n",
      "29/29 [==============================] - 4s 131ms/step - loss: nan - val_loss: nan\n",
      "Epoch 137/200\n",
      "29/29 [==============================] - 4s 127ms/step - loss: nan - val_loss: nan\n",
      "Epoch 138/200\n",
      "29/29 [==============================] - 4s 143ms/step - loss: nan - val_loss: nan\n",
      "Epoch 139/200\n",
      "29/29 [==============================] - 4s 122ms/step - loss: nan - val_loss: nan\n",
      "Epoch 140/200\n",
      "29/29 [==============================] - 3s 119ms/step - loss: nan - val_loss: nan\n",
      "Epoch 141/200\n",
      "29/29 [==============================] - 3s 119ms/step - loss: nan - val_loss: nan\n",
      "Epoch 142/200\n",
      "29/29 [==============================] - 3s 118ms/step - loss: nan - val_loss: nan\n",
      "Epoch 143/200\n",
      "29/29 [==============================] - 3s 118ms/step - loss: nan - val_loss: nan\n",
      "Epoch 144/200\n",
      "29/29 [==============================] - 4s 126ms/step - loss: nan - val_loss: nan\n",
      "Epoch 145/200\n",
      "29/29 [==============================] - 3s 118ms/step - loss: nan - val_loss: nan\n",
      "Epoch 146/200\n",
      "29/29 [==============================] - 4s 122ms/step - loss: nan - val_loss: nan\n",
      "Epoch 147/200\n",
      "29/29 [==============================] - 4s 125ms/step - loss: nan - val_loss: nan\n",
      "Epoch 148/200\n",
      "29/29 [==============================] - 3s 118ms/step - loss: nan - val_loss: nan\n",
      "Epoch 149/200\n",
      "29/29 [==============================] - 3s 117ms/step - loss: nan - val_loss: nan\n",
      "Epoch 150/200\n",
      "29/29 [==============================] - 3s 118ms/step - loss: nan - val_loss: nan\n",
      "Epoch 151/200\n",
      "29/29 [==============================] - 3s 118ms/step - loss: nan - val_loss: nan\n",
      "Epoch 152/200\n",
      "29/29 [==============================] - 3s 118ms/step - loss: nan - val_loss: nan\n",
      "Epoch 153/200\n",
      "29/29 [==============================] - 3s 119ms/step - loss: nan - val_loss: nan\n",
      "Epoch 154/200\n",
      "29/29 [==============================] - 4s 135ms/step - loss: nan - val_loss: nan\n",
      "Epoch 155/200\n",
      "29/29 [==============================] - 4s 139ms/step - loss: nan - val_loss: nan\n",
      "Epoch 156/200\n",
      "29/29 [==============================] - 4s 139ms/step - loss: nan - val_loss: nan\n",
      "Epoch 157/200\n",
      "29/29 [==============================] - 4s 150ms/step - loss: nan - val_loss: nan\n",
      "Epoch 158/200\n",
      "29/29 [==============================] - 4s 146ms/step - loss: nan - val_loss: nan\n",
      "Epoch 159/200\n",
      "29/29 [==============================] - 4s 145ms/step - loss: nan - val_loss: nan\n",
      "Epoch 160/200\n",
      "29/29 [==============================] - 4s 148ms/step - loss: nan - val_loss: nan\n",
      "Epoch 161/200\n",
      "29/29 [==============================] - 4s 146ms/step - loss: nan - val_loss: nan\n",
      "Epoch 162/200\n",
      "29/29 [==============================] - 4s 145ms/step - loss: nan - val_loss: nan\n",
      "Epoch 163/200\n",
      "29/29 [==============================] - 4s 145ms/step - loss: nan - val_loss: nan\n",
      "Epoch 164/200\n",
      "29/29 [==============================] - 4s 141ms/step - loss: nan - val_loss: nan\n",
      "Epoch 165/200\n",
      "29/29 [==============================] - 4s 141ms/step - loss: nan - val_loss: nan\n",
      "Epoch 166/200\n",
      "29/29 [==============================] - 4s 147ms/step - loss: nan - val_loss: nan\n",
      "Epoch 167/200\n",
      "29/29 [==============================] - 4s 141ms/step - loss: nan - val_loss: nan\n",
      "Epoch 168/200\n",
      "29/29 [==============================] - 4s 144ms/step - loss: nan - val_loss: nan\n",
      "Epoch 169/200\n",
      "29/29 [==============================] - 5s 166ms/step - loss: nan - val_loss: nan\n",
      "Epoch 170/200\n",
      "29/29 [==============================] - 5s 154ms/step - loss: nan - val_loss: nan\n",
      "Epoch 171/200\n",
      "29/29 [==============================] - 4s 143ms/step - loss: nan - val_loss: nan\n",
      "Epoch 172/200\n",
      "29/29 [==============================] - 4s 143ms/step - loss: nan - val_loss: nan\n",
      "Epoch 173/200\n",
      "29/29 [==============================] - 4s 141ms/step - loss: nan - val_loss: nan\n",
      "Epoch 174/200\n",
      "29/29 [==============================] - 4s 141ms/step - loss: nan - val_loss: nan\n",
      "Epoch 175/200\n",
      "29/29 [==============================] - 4s 142ms/step - loss: nan - val_loss: nan\n",
      "Epoch 176/200\n",
      "29/29 [==============================] - 4s 142ms/step - loss: nan - val_loss: nan\n",
      "Epoch 177/200\n",
      "29/29 [==============================] - 4s 143ms/step - loss: nan - val_loss: nan\n",
      "Epoch 178/200\n",
      "29/29 [==============================] - 4s 143ms/step - loss: nan - val_loss: nan\n",
      "Epoch 179/200\n",
      "29/29 [==============================] - 4s 137ms/step - loss: nan - val_loss: nan\n",
      "Epoch 180/200\n",
      "29/29 [==============================] - 4s 136ms/step - loss: nan - val_loss: nan\n",
      "Epoch 181/200\n",
      "29/29 [==============================] - 4s 137ms/step - loss: nan - val_loss: nan\n",
      "Epoch 182/200\n",
      "29/29 [==============================] - 4s 139ms/step - loss: nan - val_loss: nan\n",
      "Epoch 183/200\n",
      "29/29 [==============================] - 4s 137ms/step - loss: nan - val_loss: nan\n",
      "Epoch 184/200\n",
      "29/29 [==============================] - 4s 148ms/step - loss: nan - val_loss: nan\n",
      "Epoch 185/200\n",
      "29/29 [==============================] - 4s 141ms/step - loss: nan - val_loss: nan\n",
      "Epoch 186/200\n",
      "29/29 [==============================] - 4s 141ms/step - loss: nan - val_loss: nan\n",
      "Epoch 187/200\n",
      "29/29 [==============================] - 4s 141ms/step - loss: nan - val_loss: nan\n",
      "Epoch 188/200\n",
      "29/29 [==============================] - 4s 140ms/step - loss: nan - val_loss: nan\n",
      "Epoch 189/200\n",
      "29/29 [==============================] - 4s 137ms/step - loss: nan - val_loss: nan\n",
      "Epoch 190/200\n",
      "29/29 [==============================] - 4s 137ms/step - loss: nan - val_loss: nan\n",
      "Epoch 191/200\n",
      "29/29 [==============================] - 4s 141ms/step - loss: nan - val_loss: nan\n",
      "Epoch 192/200\n",
      "29/29 [==============================] - 4s 139ms/step - loss: nan - val_loss: nan\n",
      "Epoch 193/200\n",
      "29/29 [==============================] - 4s 147ms/step - loss: nan - val_loss: nan\n",
      "Epoch 194/200\n",
      "29/29 [==============================] - 4s 140ms/step - loss: nan - val_loss: nan\n",
      "Epoch 195/200\n",
      "29/29 [==============================] - 4s 141ms/step - loss: nan - val_loss: nan\n",
      "Epoch 196/200\n",
      "29/29 [==============================] - 4s 137ms/step - loss: nan - val_loss: nan\n",
      "Epoch 197/200\n",
      "29/29 [==============================] - 4s 141ms/step - loss: nan - val_loss: nan\n",
      "Epoch 198/200\n",
      "29/29 [==============================] - 4s 140ms/step - loss: nan - val_loss: nan\n",
      "Epoch 199/200\n",
      "29/29 [==============================] - 4s 124ms/step - loss: nan - val_loss: nan\n",
      "Epoch 200/200\n",
      "29/29 [==============================] - 3s 119ms/step - loss: nan - val_loss: nan\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2328fbeeeb0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_no_embeddings.fit(X_train_list, Y_train.to_numpy(), batch_size=64, epochs=200, validation_data=(X_val_list, Y_val.to_numpy()), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(sentences, padding='pre'):\n",
    "    encoder_inputs = [\n",
    "        [word_to_id.get(token, word_to_id['<OTHER>']) for token in sentence] for sentence in sentences\n",
    "    ]\n",
    "    encoder_inputs_padded = pad_sequences(\n",
    "        encoder_inputs, padding=padding, maxlen=padding_size_encoder, value=word_to_id['<PAD>'])\n",
    "    return encoder_inputs_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(sentences):\n",
    "    out_sentences = []\n",
    "\n",
    "    for sent in sentences:\n",
    "        out_sentences.append(' '.join([id_to_word[s] for s in sent if s != word_to_id['<PAD>']]))\n",
    "\n",
    "    return out_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [<START>, unit, generally, quite, accurate, se...\n",
       "1      [<START>, garmin, seems, generally, accurate, ...\n",
       "2      [<START>, accurate, even, destination, time, <...\n",
       "3      [<START>, accurate, travel, destination, time,...\n",
       "4      [<START>, accurate, fast, simple, operations, ...\n",
       "                             ...                        \n",
       "233    [<START>, voice, bit, robotic, voice, clear, l...\n",
       "234    [<START>, voices, sound, robotic, tts, mode, p...\n",
       "235    [<START>, 255w, garmin, gps, 750, voices, soun...\n",
       "236    [<START>, voice, clear, sweet, voice, commands...\n",
       "237                 [<START>, voice, clear, loud, <END>]\n",
       "Name: summary_tokenized_filtered, Length: 238, dtype: object"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw['summary_tokenized_filtered']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  87,  298,  178, ..., 2936, 2936, 2936],\n",
       "       [ 336,    6,   17, ..., 2936, 2936, 2936],\n",
       "       [2935,  173,    3, ...,    3,   35,    4],\n",
       "       ...,\n",
       "       [ 111,   31,   11, ..., 2936, 2936, 2936],\n",
       "       [ 110,  169,   12, ..., 2936, 2936, 2936],\n",
       "       [ 491,  432,   79, ..., 2936, 2936, 2936]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_encoded = encode(x_test, padding='post')\n",
    "x_test_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2936, 2936, 2936, ..., 1370, 1965, 2934],\n",
       "       [2936, 2936, 2936, ..., 2665,   98, 2934],\n",
       "       [2936, 2936, 2936, ..., 1151, 1076, 2934],\n",
       "       ...,\n",
       "       [2936, 2936, 2936, ..., 1364,  149, 2934],\n",
       "       [2936, 2936, 2936, ..., 1100,  242, 2934],\n",
       "       [2936, 2936, 2936, ...,  314,  150, 2934]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_encoded = encode(y_test)\n",
    "y_test_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115    [parking, expensive, think, common, san, fran,...\n",
       "15     [short, battery, life, moved, 8gb, love, ipod,...\n",
       "212    [staff, friendly, helpful, staff, morning, see...\n",
       "126    [eee, super, hybrid, engine, utility, lets, us...\n",
       "6      [room, overly, big, clean, comfortable, beds, ...\n",
       "170    [keep, mind, get, room, full, light, step, out...\n",
       "9      [room, overly, big, clean, comfortable, beds, ...\n",
       "222    [slowing, transmission, kicked, speed, wonder,...\n",
       "112    [fact, entire, navigation, structure, complete...\n",
       "221    [slowing, transmission, kicked, speed, wonder,...\n",
       "183    [customer, oriented, hotelvery, low, service, ...\n",
       "137    [previously, owned, toyota, 4runner, incredibl...\n",
       "30     [drivers, seat, comfortable, car, compared, mo...\n",
       "190    [things, 'd, like, point, must, push, micro, s...\n",
       "113    [fact, entire, navigation, structure, complete...\n",
       "55     [uninstall, anti, virus, selected, programs, l...\n",
       "24     [6ghz, 533fsb, cpu, glossy, display, 3, cell, ...\n",
       "204    [another, feature, 255w, display, posted, spee...\n",
       "86     [first, interior, way, many, cheap, plastic, p...\n",
       "19     [short, battery, life, moved, 8gb, love, ipod,...\n",
       "207    [windows, 7, quite, simply, faster, stable, bo...\n",
       "120    [happy, 08, accord, performance, quite, adequa...\n",
       "141    [great, location, nice, rooms, helpless, conci...\n",
       "235    [voice, prompts, maps, wonderful, especially, ...\n",
       "10     [plugged, usb, hub, computer, charge, battery,...\n",
       "219    [staff, swissotel, particularly, nice, time, w...\n",
       "173    [front, seats, uncomfortable, memory, seats, t...\n",
       "109    ['s, quiet, get, good, gas, mileage, looks, cl...\n",
       "75     [wine, reception, great, idea, nice, meet, tra...\n",
       "25     [thought, would, fitting, christen, kindle, st...\n",
       "124    [eee, super, hybrid, engine, utility, lets, us...\n",
       "186    [mediocre, room, service, extravagant, price, ...\n",
       "180    [us, worked, tourism, 14, years, disappointed,...\n",
       "18     [short, battery, life, moved, 8gb, love, ipod,...\n",
       "68     [food, event, delicious, food, lounge, great, ...\n",
       "60     [able, change, font, sizes, awesome, whatever,...\n",
       "148    [swissotel, one, favorite, hotels, chicago, co...\n",
       "205    [windows, 7, quite, simply, faster, stable, bo...\n",
       "114    [parking, expensive, think, common, san, fran,...\n",
       "73     [wine, reception, great, idea, nice, meet, tra...\n",
       "82     [love, new, body, style, interior, simple, ple...\n",
       "45     [3, quot, widescreen, display, bonus, made, sm...\n",
       "16     [short, battery, life, moved, 8gb, love, ipod,...\n",
       "93     [think, new, keyboard, rivals, great, hp, mini...\n",
       "199    [headphone, jack, got, clear, case, got, clear...\n",
       "167    [always, video, screen, sharp, bright, 2, inch...\n",
       "38     [ride, seems, comfortable, gas, mileage, fairl...\n",
       "127    [case, included, kindle, 1, would, reflected, ...\n",
       "Name: topic_tokenized_filtered, dtype: object"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(model, input_sent, word_to_id, padding_size):\n",
    "    generated_sent = [word_to_id['<START>']]\n",
    "\n",
    "    for i in range(padding_size):\n",
    "        output_sent = pad_sequences([generated_sent], padding_size)\n",
    "        predictions = model.predict([np.expand_dims(input_sent, axis=0), output_sent])\n",
    "        next_word = np.argmax(predictions)\n",
    "        generated_sent.append(next_word)\n",
    "\n",
    "    return generated_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [03:50<00:00,  4.79s/it]\n"
     ]
    }
   ],
   "source": [
    "output_rephrases = []\n",
    "for sentence in tqdm.tqdm(x_test_encoded):\n",
    "    output_rephrases.append(decode(model_no_embeddings, sentence, word_to_id, padding_size_decoder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(output_rephrases):\n",
    "    input_sentences = x_test\n",
    "    gt_rephrases = y_test.apply(lambda x: ' '.join(x))\n",
    "    pred_rephrases = convert(output_rephrases)\n",
    "    predictions = []\n",
    "    for input_sent, gt_rephrase, pred_rephrase in zip(input_sentences, gt_rephrases, pred_rephrases):\n",
    "        predictions.append({'gt': gt_rephrase, 'predicted': pred_rephrase})\n",
    "    return score_predictions(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating ROUGE...\n",
      "Calculating BLEU...\n",
      "{'testlen': 3456, 'reflen': 543, 'guess': [3456, 3408, 3360, 3312], 'correct': [49, 0, 0, 0]}\n",
      "ratio: 6.364640883966179\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ROUGE': (0.028329206249605124,\n",
       "  array([0.0256734 , 0.02921456, 0.0297271 , 0.02732975, 0.02732975,\n",
       "         0.0297271 , 0.02732975, 0.02921456, 0.03080808, 0.0287194 ,\n",
       "         0.02824074, 0.02824074, 0.02455717, 0.0249183 , 0.02647569,\n",
       "         0.02921456, 0.02647569, 0.02732975, 0.02824074, 0.0297271 ,\n",
       "         0.02689594, 0.02777778, 0.03025794, 0.02824074, 0.02777778,\n",
       "         0.02777778, 0.02647569, 0.02353395, 0.0249183 , 0.0297271 ,\n",
       "         0.0287194 , 0.05648148, 0.02689594, 0.02824074, 0.02777778,\n",
       "         0.02689594, 0.02777778, 0.02455717, 0.02921456, 0.02647569,\n",
       "         0.0297271 , 0.02777778, 0.0249183 , 0.0297271 , 0.02921456,\n",
       "         0.02824074, 0.02921456, 0.02606838])),\n",
       " 'BLEU-1': (0.014178240740736639,\n",
       "  [0.013888888888696002,\n",
       "   0.013888888888696002,\n",
       "   0.013888888888696002,\n",
       "   0.013888888888696002,\n",
       "   0.013888888888696002,\n",
       "   0.013888888888696002,\n",
       "   0.013888888888696002,\n",
       "   0.013888888888696002,\n",
       "   0.013888888888696002,\n",
       "   0.013888888888696002,\n",
       "   0.013888888888696002,\n",
       "   0.013888888888696002,\n",
       "   0.013888888888696002,\n",
       "   0.013888888888696002,\n",
       "   0.013888888888696002,\n",
       "   0.013888888888696002,\n",
       "   0.013888888888696002,\n",
       "   0.013888888888696002,\n",
       "   0.013888888888696002,\n",
       "   0.013888888888696002,\n",
       "   0.013888888888696002,\n",
       "   0.013888888888696002,\n",
       "   0.013888888888696002,\n",
       "   0.013888888888696002,\n",
       "   0.013888888888696002,\n",
       "   0.013888888888696002,\n",
       "   0.013888888888696002,\n",
       "   0.013888888888696002,\n",
       "   0.013888888888696002,\n",
       "   0.013888888888696002,\n",
       "   0.013888888888696002,\n",
       "   0.027777777777391988,\n",
       "   0.013888888888696002,\n",
       "   0.013888888888696002,\n",
       "   0.013888888888696002,\n",
       "   0.013888888888696002,\n",
       "   0.013888888888696002,\n",
       "   0.013888888888696002,\n",
       "   0.013888888888696002,\n",
       "   0.013888888888696002,\n",
       "   0.013888888888696002,\n",
       "   0.013888888888696002,\n",
       "   0.013888888888696002,\n",
       "   0.013888888888696002,\n",
       "   0.013888888888696002,\n",
       "   0.013888888888696002,\n",
       "   0.013888888888696002,\n",
       "   0.013888888888696002]),\n",
       " 'BLEU-2': (6.450024734442439e-11,\n",
       "  [4.422874103557102e-10,\n",
       "   4.422874103557102e-10,\n",
       "   4.422874103557102e-10,\n",
       "   4.422874103557102e-10,\n",
       "   4.422874103557102e-10,\n",
       "   4.422874103557102e-10,\n",
       "   4.422874103557102e-10,\n",
       "   4.422874103557102e-10,\n",
       "   4.422874103557102e-10,\n",
       "   4.422874103557102e-10,\n",
       "   4.422874103557102e-10,\n",
       "   4.422874103557102e-10,\n",
       "   4.422874103557102e-10,\n",
       "   4.422874103557102e-10,\n",
       "   4.422874103557102e-10,\n",
       "   4.422874103557102e-10,\n",
       "   4.422874103557102e-10,\n",
       "   4.422874103557102e-10,\n",
       "   4.422874103557102e-10,\n",
       "   4.422874103557102e-10,\n",
       "   4.422874103557102e-10,\n",
       "   4.422874103557102e-10,\n",
       "   4.422874103557102e-10,\n",
       "   4.422874103557102e-10,\n",
       "   4.422874103557102e-10,\n",
       "   4.422874103557102e-10,\n",
       "   4.422874103557102e-10,\n",
       "   4.422874103557102e-10,\n",
       "   4.422874103557102e-10,\n",
       "   4.422874103557102e-10,\n",
       "   4.422874103557102e-10,\n",
       "   6.254888541919197e-10,\n",
       "   4.422874103557102e-10,\n",
       "   4.422874103557102e-10,\n",
       "   4.422874103557102e-10,\n",
       "   4.422874103557102e-10,\n",
       "   4.422874103557102e-10,\n",
       "   4.422874103557102e-10,\n",
       "   4.422874103557102e-10,\n",
       "   4.422874103557102e-10,\n",
       "   4.422874103557102e-10,\n",
       "   4.422874103557102e-10,\n",
       "   4.422874103557102e-10,\n",
       "   4.422874103557102e-10,\n",
       "   4.422874103557102e-10,\n",
       "   4.422874103557102e-10,\n",
       "   4.422874103557102e-10,\n",
       "   4.422874103557102e-10]),\n",
       " 'BLEU-3': (1.0738109482366869e-13,\n",
       "  [1.4085438495469813e-12,\n",
       "   1.4085438495469813e-12,\n",
       "   1.4085438495469813e-12,\n",
       "   1.4085438495469813e-12,\n",
       "   1.4085438495469813e-12,\n",
       "   1.4085438495469813e-12,\n",
       "   1.4085438495469813e-12,\n",
       "   1.4085438495469813e-12,\n",
       "   1.4085438495469813e-12,\n",
       "   1.4085438495469813e-12,\n",
       "   1.4085438495469813e-12,\n",
       "   1.4085438495469813e-12,\n",
       "   1.4085438495469813e-12,\n",
       "   1.4085438495469813e-12,\n",
       "   1.4085438495469813e-12,\n",
       "   1.4085438495469813e-12,\n",
       "   1.4085438495469813e-12,\n",
       "   1.4085438495469813e-12,\n",
       "   1.4085438495469813e-12,\n",
       "   1.4085438495469813e-12,\n",
       "   1.4085438495469813e-12,\n",
       "   1.4085438495469813e-12,\n",
       "   1.4085438495469813e-12,\n",
       "   1.4085438495469813e-12,\n",
       "   1.4085438495469813e-12,\n",
       "   1.4085438495469813e-12,\n",
       "   1.4085438495469813e-12,\n",
       "   1.4085438495469813e-12,\n",
       "   1.4085438495469813e-12,\n",
       "   1.4085438495469813e-12,\n",
       "   1.4085438495469813e-12,\n",
       "   1.7746540457441984e-12,\n",
       "   1.4085438495469813e-12,\n",
       "   1.4085438495469813e-12,\n",
       "   1.4085438495469813e-12,\n",
       "   1.4085438495469813e-12,\n",
       "   1.4085438495469813e-12,\n",
       "   1.4085438495469813e-12,\n",
       "   1.4085438495469813e-12,\n",
       "   1.4085438495469813e-12,\n",
       "   1.4085438495469813e-12,\n",
       "   1.4085438495469813e-12,\n",
       "   1.4085438495469813e-12,\n",
       "   1.4085438495469813e-12,\n",
       "   1.4085438495469813e-12,\n",
       "   1.4085438495469813e-12,\n",
       "   1.4085438495469813e-12,\n",
       "   1.4085438495469813e-12]),\n",
       " 'BLEU-4': (4.397170327568379e-15,\n",
       "  [7.977475994726905e-14,\n",
       "   7.977475994726905e-14,\n",
       "   7.977475994726905e-14,\n",
       "   7.977475994726905e-14,\n",
       "   7.977475994726905e-14,\n",
       "   7.977475994726905e-14,\n",
       "   7.977475994726905e-14,\n",
       "   7.977475994726905e-14,\n",
       "   7.977475994726905e-14,\n",
       "   7.977475994726905e-14,\n",
       "   7.977475994726905e-14,\n",
       "   7.977475994726905e-14,\n",
       "   7.977475994726905e-14,\n",
       "   7.977475994726905e-14,\n",
       "   7.977475994726905e-14,\n",
       "   7.977475994726905e-14,\n",
       "   7.977475994726905e-14,\n",
       "   7.977475994726905e-14,\n",
       "   7.977475994726905e-14,\n",
       "   7.977475994726905e-14,\n",
       "   7.977475994726905e-14,\n",
       "   7.977475994726905e-14,\n",
       "   7.977475994726905e-14,\n",
       "   7.977475994726905e-14,\n",
       "   7.977475994726905e-14,\n",
       "   7.977475994726905e-14,\n",
       "   7.977475994726905e-14,\n",
       "   7.977475994726905e-14,\n",
       "   7.977475994726905e-14,\n",
       "   7.977475994726905e-14,\n",
       "   7.977475994726905e-14,\n",
       "   9.486871212692643e-14,\n",
       "   7.977475994726905e-14,\n",
       "   7.977475994726905e-14,\n",
       "   7.977475994726905e-14,\n",
       "   7.977475994726905e-14,\n",
       "   7.977475994726905e-14,\n",
       "   7.977475994726905e-14,\n",
       "   7.977475994726905e-14,\n",
       "   7.977475994726905e-14,\n",
       "   7.977475994726905e-14,\n",
       "   7.977475994726905e-14,\n",
       "   7.977475994726905e-14,\n",
       "   7.977475994726905e-14,\n",
       "   7.977475994726905e-14,\n",
       "   7.977475994726905e-14,\n",
       "   7.977475994726905e-14,\n",
       "   7.977475994726905e-14])}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_results(output_rephrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is edited in word_embeddings.py by me to accept path argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_weights = load_embedding_weights(\n",
    "    vocabulary=smaller_voc,\n",
    "    embedding_size=50,\n",
    "    embedding_type='glove',\n",
    "    path='data'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_glove = create_model(padding_size_encoder, padding_size_decoder, len(smaller_voc), 50, glove_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "29/29 [==============================] - 9s 173ms/step - loss: 6.3361 - val_loss: 6.1971\n",
      "Epoch 2/200\n",
      "29/29 [==============================] - 4s 135ms/step - loss: 5.1903 - val_loss: 6.1989\n",
      "Epoch 3/200\n",
      "29/29 [==============================] - 4s 133ms/step - loss: 4.6427 - val_loss: 6.1637\n",
      "Epoch 4/200\n",
      "29/29 [==============================] - 4s 139ms/step - loss: 3.8989 - val_loss: 6.1327\n",
      "Epoch 5/200\n",
      "29/29 [==============================] - 4s 139ms/step - loss: 3.0611 - val_loss: 6.2775\n",
      "Epoch 6/200\n",
      "29/29 [==============================] - 4s 141ms/step - loss: 2.3136 - val_loss: 6.4727\n",
      "Epoch 7/200\n",
      "29/29 [==============================] - 4s 134ms/step - loss: 1.7603 - val_loss: 6.6220\n",
      "Epoch 8/200\n",
      "29/29 [==============================] - 4s 134ms/step - loss: 1.3731 - val_loss: 6.7986\n",
      "Epoch 9/200\n",
      "29/29 [==============================] - 4s 134ms/step - loss: 1.1517 - val_loss: 6.9304\n",
      "Epoch 10/200\n",
      "29/29 [==============================] - 4s 137ms/step - loss: 0.9718 - val_loss: 7.0338\n",
      "Epoch 11/200\n",
      "29/29 [==============================] - 4s 136ms/step - loss: 0.8991 - val_loss: 7.0271\n",
      "Epoch 12/200\n",
      "29/29 [==============================] - 4s 133ms/step - loss: 0.8357 - val_loss: 7.1488\n",
      "Epoch 13/200\n",
      "29/29 [==============================] - 4s 134ms/step - loss: 0.8132 - val_loss: 7.2012\n",
      "Epoch 14/200\n",
      "29/29 [==============================] - 4s 133ms/step - loss: 0.7843 - val_loss: 7.2080\n",
      "Epoch 15/200\n",
      "29/29 [==============================] - 4s 137ms/step - loss: 0.7805 - val_loss: 7.1345\n",
      "Epoch 16/200\n",
      "29/29 [==============================] - 4s 134ms/step - loss: 0.7538 - val_loss: 7.2458\n",
      "Epoch 17/200\n",
      "29/29 [==============================] - 4s 137ms/step - loss: 0.7543 - val_loss: 7.2417\n",
      "Epoch 18/200\n",
      "29/29 [==============================] - 4s 138ms/step - loss: 0.8057 - val_loss: 7.2997\n",
      "Epoch 19/200\n",
      "29/29 [==============================] - 4s 133ms/step - loss: 0.7627 - val_loss: 7.4328\n",
      "Epoch 20/200\n",
      "29/29 [==============================] - 4s 134ms/step - loss: 0.7435 - val_loss: 7.4716\n",
      "Epoch 21/200\n",
      "29/29 [==============================] - 4s 135ms/step - loss: 0.7224 - val_loss: 7.5469\n",
      "Epoch 22/200\n",
      "29/29 [==============================] - 4s 135ms/step - loss: 0.7414 - val_loss: 7.6297\n",
      "Epoch 23/200\n",
      "29/29 [==============================] - 4s 138ms/step - loss: 0.6804 - val_loss: 7.6658\n",
      "Epoch 24/200\n",
      "29/29 [==============================] - 4s 129ms/step - loss: 0.7171 - val_loss: 7.8387\n",
      "Epoch 25/200\n",
      "29/29 [==============================] - 3s 117ms/step - loss: 0.6870 - val_loss: 8.0030\n",
      "Epoch 26/200\n",
      "29/29 [==============================] - 3s 117ms/step - loss: 0.6874 - val_loss: 8.0535\n",
      "Epoch 27/200\n",
      "29/29 [==============================] - 3s 117ms/step - loss: 0.6724 - val_loss: 8.1411\n",
      "Epoch 28/200\n",
      "29/29 [==============================] - 3s 118ms/step - loss: 0.6828 - val_loss: 8.3114\n",
      "Epoch 29/200\n",
      "29/29 [==============================] - 4s 143ms/step - loss: 0.6757 - val_loss: 8.3685\n",
      "Epoch 30/200\n",
      "29/29 [==============================] - 4s 146ms/step - loss: 0.6766 - val_loss: 8.6570\n",
      "Epoch 31/200\n",
      "29/29 [==============================] - 4s 123ms/step - loss: 0.7117 - val_loss: 8.5994\n",
      "Epoch 32/200\n",
      "29/29 [==============================] - 4s 139ms/step - loss: 1.0540 - val_loss: 8.6812\n",
      "Epoch 33/200\n",
      "29/29 [==============================] - 4s 132ms/step - loss: 0.8820 - val_loss: 8.3569\n",
      "Epoch 34/200\n",
      "29/29 [==============================] - 4s 132ms/step - loss: 0.7826 - val_loss: 8.6939\n",
      "Epoch 35/200\n",
      "29/29 [==============================] - 4s 126ms/step - loss: 0.6764 - val_loss: 8.6995\n",
      "Epoch 36/200\n",
      "29/29 [==============================] - 4s 127ms/step - loss: 0.8979 - val_loss: 8.9352\n",
      "Epoch 37/200\n",
      "29/29 [==============================] - 4s 126ms/step - loss: 0.7132 - val_loss: 8.8383\n",
      "Epoch 38/200\n",
      "29/29 [==============================] - 4s 128ms/step - loss: 0.6562 - val_loss: 8.9226\n",
      "Epoch 39/200\n",
      "29/29 [==============================] - 4s 126ms/step - loss: 0.6471 - val_loss: 9.0618\n",
      "Epoch 40/200\n",
      "29/29 [==============================] - 4s 126ms/step - loss: 0.5909 - val_loss: 9.0747\n",
      "Epoch 41/200\n",
      "29/29 [==============================] - 4s 129ms/step - loss: 0.5522 - val_loss: 9.1529\n",
      "Epoch 42/200\n",
      "29/29 [==============================] - 4s 129ms/step - loss: 0.5513 - val_loss: 9.2894\n",
      "Epoch 43/200\n",
      "29/29 [==============================] - 4s 126ms/step - loss: 0.5429 - val_loss: 9.3857\n",
      "Epoch 44/200\n",
      "29/29 [==============================] - 4s 125ms/step - loss: 0.5111 - val_loss: 9.4626\n",
      "Epoch 45/200\n",
      "29/29 [==============================] - 4s 126ms/step - loss: 0.5551 - val_loss: 9.5736\n",
      "Epoch 46/200\n",
      "29/29 [==============================] - 4s 126ms/step - loss: 0.5329 - val_loss: 9.5616\n",
      "Epoch 47/200\n",
      "29/29 [==============================] - 4s 127ms/step - loss: 0.5450 - val_loss: 9.6191\n",
      "Epoch 48/200\n",
      "29/29 [==============================] - 4s 126ms/step - loss: 0.5312 - val_loss: 9.7219\n",
      "Epoch 49/200\n",
      "29/29 [==============================] - 4s 126ms/step - loss: 0.5483 - val_loss: 9.8343\n",
      "Epoch 50/200\n",
      "29/29 [==============================] - 4s 126ms/step - loss: 0.5479 - val_loss: 9.7765\n",
      "Epoch 51/200\n",
      "29/29 [==============================] - 4s 127ms/step - loss: 0.5391 - val_loss: 10.1367\n",
      "Epoch 52/200\n",
      "29/29 [==============================] - 4s 127ms/step - loss: 0.5329 - val_loss: 10.2329\n",
      "Epoch 53/200\n",
      "29/29 [==============================] - 4s 126ms/step - loss: 0.5724 - val_loss: 10.2151\n",
      "Epoch 54/200\n",
      "29/29 [==============================] - 4s 126ms/step - loss: 0.6273 - val_loss: 10.3498\n",
      "Epoch 55/200\n",
      "29/29 [==============================] - 4s 130ms/step - loss: 0.6225 - val_loss: 10.4818\n",
      "Epoch 56/200\n",
      "29/29 [==============================] - 4s 128ms/step - loss: 0.6130 - val_loss: 10.4747\n",
      "Epoch 57/200\n",
      "29/29 [==============================] - 4s 129ms/step - loss: 0.6617 - val_loss: 10.7577\n",
      "Epoch 58/200\n",
      "29/29 [==============================] - 4s 129ms/step - loss: 0.6457 - val_loss: 10.4637\n",
      "Epoch 59/200\n",
      "29/29 [==============================] - 4s 126ms/step - loss: 0.5989 - val_loss: 10.8342\n",
      "Epoch 60/200\n",
      "29/29 [==============================] - 4s 128ms/step - loss: 0.6021 - val_loss: 10.4422\n",
      "Epoch 61/200\n",
      "29/29 [==============================] - 4s 128ms/step - loss: 0.6101 - val_loss: 10.7663\n",
      "Epoch 62/200\n",
      "29/29 [==============================] - 3s 118ms/step - loss: 0.6488 - val_loss: 10.7328\n",
      "Epoch 63/200\n",
      "29/29 [==============================] - 3s 120ms/step - loss: 0.6304 - val_loss: 10.7741\n",
      "Epoch 64/200\n",
      "29/29 [==============================] - 3s 112ms/step - loss: 0.6755 - val_loss: 11.4644\n",
      "Epoch 65/200\n",
      "29/29 [==============================] - 3s 113ms/step - loss: 0.7330 - val_loss: 11.2794\n",
      "Epoch 66/200\n",
      "29/29 [==============================] - 3s 113ms/step - loss: 0.7453 - val_loss: 11.3941\n",
      "Epoch 67/200\n",
      "29/29 [==============================] - 3s 112ms/step - loss: 0.7682 - val_loss: 11.2839\n",
      "Epoch 68/200\n",
      "29/29 [==============================] - 3s 111ms/step - loss: 0.9707 - val_loss: 11.5003\n",
      "Epoch 69/200\n",
      "29/29 [==============================] - 3s 112ms/step - loss: 1.0252 - val_loss: 12.4696\n",
      "Epoch 70/200\n",
      "29/29 [==============================] - 3s 112ms/step - loss: 1.0653 - val_loss: 12.0680\n",
      "Epoch 71/200\n",
      "29/29 [==============================] - 3s 112ms/step - loss: 1.0362 - val_loss: 12.1613\n",
      "Epoch 72/200\n",
      "29/29 [==============================] - 3s 113ms/step - loss: nan - val_loss: nan\n",
      "Epoch 73/200\n",
      "29/29 [==============================] - 3s 111ms/step - loss: nan - val_loss: nan\n",
      "Epoch 74/200\n",
      "29/29 [==============================] - 3s 111ms/step - loss: nan - val_loss: nan\n",
      "Epoch 75/200\n",
      "29/29 [==============================] - 3s 112ms/step - loss: nan - val_loss: nan\n",
      "Epoch 76/200\n",
      "29/29 [==============================] - 3s 111ms/step - loss: nan - val_loss: nan\n",
      "Epoch 77/200\n",
      "29/29 [==============================] - 3s 112ms/step - loss: nan - val_loss: nan\n",
      "Epoch 78/200\n",
      "29/29 [==============================] - 3s 111ms/step - loss: nan - val_loss: nan\n",
      "Epoch 79/200\n",
      "29/29 [==============================] - 3s 112ms/step - loss: nan - val_loss: nan\n",
      "Epoch 80/200\n",
      "29/29 [==============================] - 3s 111ms/step - loss: nan - val_loss: nan\n",
      "Epoch 81/200\n",
      "29/29 [==============================] - 3s 112ms/step - loss: nan - val_loss: nan\n",
      "Epoch 82/200\n",
      "29/29 [==============================] - 3s 111ms/step - loss: nan - val_loss: nan\n",
      "Epoch 83/200\n",
      "29/29 [==============================] - 3s 111ms/step - loss: nan - val_loss: nan\n",
      "Epoch 84/200\n",
      "29/29 [==============================] - 3s 111ms/step - loss: nan - val_loss: nan\n",
      "Epoch 85/200\n",
      "29/29 [==============================] - 3s 112ms/step - loss: nan - val_loss: nan\n",
      "Epoch 86/200\n",
      "29/29 [==============================] - 3s 112ms/step - loss: nan - val_loss: nan\n",
      "Epoch 87/200\n",
      "29/29 [==============================] - 3s 112ms/step - loss: nan - val_loss: nan\n",
      "Epoch 88/200\n",
      "29/29 [==============================] - 3s 111ms/step - loss: nan - val_loss: nan\n",
      "Epoch 89/200\n",
      "29/29 [==============================] - 3s 111ms/step - loss: nan - val_loss: nan\n",
      "Epoch 90/200\n",
      "29/29 [==============================] - 3s 111ms/step - loss: nan - val_loss: nan\n",
      "Epoch 91/200\n",
      "29/29 [==============================] - 3s 111ms/step - loss: nan - val_loss: nan\n",
      "Epoch 92/200\n",
      "29/29 [==============================] - 3s 111ms/step - loss: nan - val_loss: nan\n",
      "Epoch 93/200\n",
      "29/29 [==============================] - 3s 112ms/step - loss: nan - val_loss: nan\n",
      "Epoch 94/200\n",
      "29/29 [==============================] - 3s 110ms/step - loss: nan - val_loss: nan\n",
      "Epoch 95/200\n",
      "29/29 [==============================] - 3s 111ms/step - loss: nan - val_loss: nan\n",
      "Epoch 96/200\n",
      "29/29 [==============================] - 3s 111ms/step - loss: nan - val_loss: nan\n",
      "Epoch 97/200\n",
      "29/29 [==============================] - 3s 111ms/step - loss: nan - val_loss: nan\n",
      "Epoch 98/200\n",
      "29/29 [==============================] - 3s 112ms/step - loss: nan - val_loss: nan\n",
      "Epoch 99/200\n",
      "29/29 [==============================] - 3s 112ms/step - loss: nan - val_loss: nan\n",
      "Epoch 100/200\n",
      "29/29 [==============================] - 3s 111ms/step - loss: nan - val_loss: nan\n",
      "Epoch 101/200\n",
      "29/29 [==============================] - 3s 112ms/step - loss: nan - val_loss: nan\n",
      "Epoch 102/200\n",
      "29/29 [==============================] - 3s 111ms/step - loss: nan - val_loss: nan\n",
      "Epoch 103/200\n",
      "29/29 [==============================] - 3s 111ms/step - loss: nan - val_loss: nan\n",
      "Epoch 104/200\n",
      "29/29 [==============================] - 3s 112ms/step - loss: nan - val_loss: nan\n",
      "Epoch 105/200\n",
      "29/29 [==============================] - 3s 112ms/step - loss: nan - val_loss: nan\n",
      "Epoch 106/200\n",
      "29/29 [==============================] - 3s 112ms/step - loss: nan - val_loss: nan\n",
      "Epoch 107/200\n",
      "29/29 [==============================] - 3s 112ms/step - loss: nan - val_loss: nan\n",
      "Epoch 108/200\n",
      "29/29 [==============================] - 3s 112ms/step - loss: nan - val_loss: nan\n",
      "Epoch 109/200\n",
      "29/29 [==============================] - 3s 112ms/step - loss: nan - val_loss: nan\n",
      "Epoch 110/200\n",
      "29/29 [==============================] - 3s 112ms/step - loss: nan - val_loss: nan\n",
      "Epoch 111/200\n",
      "29/29 [==============================] - 3s 112ms/step - loss: nan - val_loss: nan\n",
      "Epoch 112/200\n",
      "29/29 [==============================] - 3s 112ms/step - loss: nan - val_loss: nan\n",
      "Epoch 113/200\n",
      "29/29 [==============================] - 3s 111ms/step - loss: nan - val_loss: nan\n",
      "Epoch 114/200\n",
      "29/29 [==============================] - 3s 112ms/step - loss: nan - val_loss: nan\n",
      "Epoch 115/200\n",
      "29/29 [==============================] - 3s 112ms/step - loss: nan - val_loss: nan\n",
      "Epoch 116/200\n",
      "29/29 [==============================] - 3s 111ms/step - loss: nan - val_loss: nan\n",
      "Epoch 117/200\n",
      "29/29 [==============================] - 3s 112ms/step - loss: nan - val_loss: nan\n",
      "Epoch 118/200\n",
      "29/29 [==============================] - 3s 111ms/step - loss: nan - val_loss: nan\n",
      "Epoch 119/200\n",
      "29/29 [==============================] - 3s 111ms/step - loss: nan - val_loss: nan\n",
      "Epoch 120/200\n",
      "29/29 [==============================] - 3s 111ms/step - loss: nan - val_loss: nan\n",
      "Epoch 121/200\n",
      "29/29 [==============================] - 3s 111ms/step - loss: nan - val_loss: nan\n",
      "Epoch 122/200\n",
      "29/29 [==============================] - 3s 112ms/step - loss: nan - val_loss: nan\n",
      "Epoch 123/200\n",
      "29/29 [==============================] - 3s 112ms/step - loss: nan - val_loss: nan\n",
      "Epoch 124/200\n",
      "29/29 [==============================] - 3s 112ms/step - loss: nan - val_loss: nan\n",
      "Epoch 125/200\n",
      "29/29 [==============================] - 3s 112ms/step - loss: nan - val_loss: nan\n",
      "Epoch 126/200\n",
      "29/29 [==============================] - 3s 111ms/step - loss: nan - val_loss: nan\n",
      "Epoch 127/200\n",
      "29/29 [==============================] - 3s 112ms/step - loss: nan - val_loss: nan\n",
      "Epoch 128/200\n",
      "29/29 [==============================] - 3s 112ms/step - loss: nan - val_loss: nan\n",
      "Epoch 129/200\n",
      "29/29 [==============================] - 3s 112ms/step - loss: nan - val_loss: nan\n",
      "Epoch 130/200\n",
      "29/29 [==============================] - 3s 111ms/step - loss: nan - val_loss: nan\n",
      "Epoch 131/200\n",
      "29/29 [==============================] - 3s 112ms/step - loss: nan - val_loss: nan\n",
      "Epoch 132/200\n",
      "29/29 [==============================] - 3s 112ms/step - loss: nan - val_loss: nan\n",
      "Epoch 133/200\n",
      "29/29 [==============================] - 3s 112ms/step - loss: nan - val_loss: nan\n",
      "Epoch 134/200\n",
      "29/29 [==============================] - 3s 111ms/step - loss: nan - val_loss: nan\n",
      "Epoch 135/200\n",
      "29/29 [==============================] - 3s 111ms/step - loss: nan - val_loss: nan\n",
      "Epoch 136/200\n",
      "29/29 [==============================] - 3s 112ms/step - loss: nan - val_loss: nan\n",
      "Epoch 137/200\n",
      "29/29 [==============================] - 3s 112ms/step - loss: nan - val_loss: nan\n",
      "Epoch 138/200\n",
      "29/29 [==============================] - 3s 111ms/step - loss: nan - val_loss: nan\n",
      "Epoch 139/200\n",
      "29/29 [==============================] - 3s 112ms/step - loss: nan - val_loss: nan\n",
      "Epoch 140/200\n",
      "29/29 [==============================] - 3s 112ms/step - loss: nan - val_loss: nan\n",
      "Epoch 141/200\n",
      "29/29 [==============================] - 3s 112ms/step - loss: nan - val_loss: nan\n",
      "Epoch 142/200\n",
      "29/29 [==============================] - 3s 112ms/step - loss: nan - val_loss: nan\n",
      "Epoch 143/200\n",
      "29/29 [==============================] - 3s 112ms/step - loss: nan - val_loss: nan\n",
      "Epoch 144/200\n",
      "29/29 [==============================] - 3s 112ms/step - loss: nan - val_loss: nan\n",
      "Epoch 145/200\n",
      "29/29 [==============================] - 3s 111ms/step - loss: nan - val_loss: nan\n",
      "Epoch 146/200\n",
      "29/29 [==============================] - 3s 111ms/step - loss: nan - val_loss: nan\n",
      "Epoch 147/200\n",
      "29/29 [==============================] - 3s 112ms/step - loss: nan - val_loss: nan\n",
      "Epoch 148/200\n",
      "29/29 [==============================] - 3s 111ms/step - loss: nan - val_loss: nan\n",
      "Epoch 149/200\n",
      "29/29 [==============================] - 3s 111ms/step - loss: nan - val_loss: nan\n",
      "Epoch 150/200\n",
      "29/29 [==============================] - 3s 112ms/step - loss: nan - val_loss: nan\n",
      "Epoch 151/200\n",
      "29/29 [==============================] - 3s 112ms/step - loss: nan - val_loss: nan\n",
      "Epoch 152/200\n",
      "29/29 [==============================] - 3s 113ms/step - loss: nan - val_loss: nan\n",
      "Epoch 153/200\n",
      "29/29 [==============================] - 3s 112ms/step - loss: nan - val_loss: nan\n",
      "Epoch 154/200\n",
      "29/29 [==============================] - 3s 111ms/step - loss: nan - val_loss: nan\n",
      "Epoch 155/200\n",
      "29/29 [==============================] - 3s 111ms/step - loss: nan - val_loss: nan\n",
      "Epoch 156/200\n",
      "29/29 [==============================] - 3s 111ms/step - loss: nan - val_loss: nan\n",
      "Epoch 157/200\n",
      "29/29 [==============================] - 3s 111ms/step - loss: nan - val_loss: nan\n",
      "Epoch 158/200\n",
      "29/29 [==============================] - 3s 111ms/step - loss: nan - val_loss: nan\n",
      "Epoch 159/200\n",
      "29/29 [==============================] - 3s 111ms/step - loss: nan - val_loss: nan\n",
      "Epoch 160/200\n",
      "29/29 [==============================] - 3s 111ms/step - loss: nan - val_loss: nan\n",
      "Epoch 161/200\n",
      "29/29 [==============================] - 3s 111ms/step - loss: nan - val_loss: nan\n",
      "Epoch 162/200\n",
      "29/29 [==============================] - 3s 111ms/step - loss: nan - val_loss: nan\n",
      "Epoch 163/200\n",
      "29/29 [==============================] - 3s 111ms/step - loss: nan - val_loss: nan\n",
      "Epoch 164/200\n",
      "29/29 [==============================] - 3s 112ms/step - loss: nan - val_loss: nan\n",
      "Epoch 165/200\n",
      "29/29 [==============================] - 3s 111ms/step - loss: nan - val_loss: nan\n",
      "Epoch 166/200\n",
      "29/29 [==============================] - 3s 111ms/step - loss: nan - val_loss: nan\n",
      "Epoch 167/200\n",
      "29/29 [==============================] - 3s 111ms/step - loss: nan - val_loss: nan\n",
      "Epoch 168/200\n",
      "29/29 [==============================] - 3s 112ms/step - loss: nan - val_loss: nan\n",
      "Epoch 169/200\n",
      "29/29 [==============================] - 3s 111ms/step - loss: nan - val_loss: nan\n",
      "Epoch 170/200\n",
      "29/29 [==============================] - 3s 112ms/step - loss: nan - val_loss: nan\n",
      "Epoch 171/200\n",
      "29/29 [==============================] - 3s 111ms/step - loss: nan - val_loss: nan\n",
      "Epoch 172/200\n",
      "29/29 [==============================] - 3s 111ms/step - loss: nan - val_loss: nan\n",
      "Epoch 173/200\n",
      "29/29 [==============================] - 3s 112ms/step - loss: nan - val_loss: nan\n",
      "Epoch 174/200\n",
      "29/29 [==============================] - 3s 111ms/step - loss: nan - val_loss: nan\n",
      "Epoch 175/200\n",
      "29/29 [==============================] - 3s 112ms/step - loss: nan - val_loss: nan\n",
      "Epoch 176/200\n",
      "29/29 [==============================] - 3s 111ms/step - loss: nan - val_loss: nan\n",
      "Epoch 177/200\n",
      "29/29 [==============================] - 3s 111ms/step - loss: nan - val_loss: nan\n",
      "Epoch 178/200\n",
      "29/29 [==============================] - 3s 111ms/step - loss: nan - val_loss: nan\n",
      "Epoch 179/200\n",
      "29/29 [==============================] - 3s 111ms/step - loss: nan - val_loss: nan\n",
      "Epoch 180/200\n",
      "29/29 [==============================] - 3s 111ms/step - loss: nan - val_loss: nan\n",
      "Epoch 181/200\n",
      "29/29 [==============================] - 3s 111ms/step - loss: nan - val_loss: nan\n",
      "Epoch 182/200\n",
      "29/29 [==============================] - 3s 111ms/step - loss: nan - val_loss: nan\n",
      "Epoch 183/200\n",
      "29/29 [==============================] - 3s 111ms/step - loss: nan - val_loss: nan\n",
      "Epoch 184/200\n",
      "29/29 [==============================] - 3s 112ms/step - loss: nan - val_loss: nan\n",
      "Epoch 185/200\n",
      "29/29 [==============================] - 3s 112ms/step - loss: nan - val_loss: nan\n",
      "Epoch 186/200\n",
      "29/29 [==============================] - 3s 111ms/step - loss: nan - val_loss: nan\n",
      "Epoch 187/200\n",
      "29/29 [==============================] - 3s 111ms/step - loss: nan - val_loss: nan\n",
      "Epoch 188/200\n",
      "29/29 [==============================] - 3s 111ms/step - loss: nan - val_loss: nan\n",
      "Epoch 189/200\n",
      "29/29 [==============================] - 3s 111ms/step - loss: nan - val_loss: nan\n",
      "Epoch 190/200\n",
      "29/29 [==============================] - 3s 111ms/step - loss: nan - val_loss: nan\n",
      "Epoch 191/200\n",
      "29/29 [==============================] - 3s 111ms/step - loss: nan - val_loss: nan\n",
      "Epoch 192/200\n",
      "29/29 [==============================] - 3s 112ms/step - loss: nan - val_loss: nan\n",
      "Epoch 193/200\n",
      "29/29 [==============================] - 3s 111ms/step - loss: nan - val_loss: nan\n",
      "Epoch 194/200\n",
      "29/29 [==============================] - 3s 112ms/step - loss: nan - val_loss: nan\n",
      "Epoch 195/200\n",
      "29/29 [==============================] - 3s 112ms/step - loss: nan - val_loss: nan\n",
      "Epoch 196/200\n",
      "29/29 [==============================] - 3s 112ms/step - loss: nan - val_loss: nan\n",
      "Epoch 197/200\n",
      "29/29 [==============================] - 3s 111ms/step - loss: nan - val_loss: nan\n",
      "Epoch 198/200\n",
      "29/29 [==============================] - 3s 111ms/step - loss: nan - val_loss: nan\n",
      "Epoch 199/200\n",
      "29/29 [==============================] - 3s 112ms/step - loss: nan - val_loss: nan\n",
      "Epoch 200/200\n",
      "29/29 [==============================] - 3s 111ms/step - loss: nan - val_loss: nan\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2331d495ca0>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_glove.fit(X_train_list, Y_train.to_numpy(), batch_size=64, epochs=200, validation_data=(X_val_list, Y_val.to_numpy()), verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model starts overfitting the last few epochs seen by the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [03:27<00:00,  4.31s/it]\n"
     ]
    }
   ],
   "source": [
    "output_rephrases_glove = []\n",
    "for sentence in tqdm.tqdm(x_test_encoded):\n",
    "    output_rephrases_glove.append(decode(model_glove, sentence, word_to_id, padding_size_decoder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating ROUGE...\n",
      "Calculating BLEU...\n",
      "{'testlen': 3456, 'reflen': 543, 'guess': [3456, 3408, 3360, 3312], 'correct': [49, 0, 0, 0]}\n",
      "ratio: 6.364640883966179\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ROUGE': (0.028329206249605124,\n",
       "  array([0.0256734 , 0.02921456, 0.0297271 , 0.02732975, 0.02732975,\n",
       "         0.0297271 , 0.02732975, 0.02921456, 0.03080808, 0.0287194 ,\n",
       "         0.02824074, 0.02824074, 0.02455717, 0.0249183 , 0.02647569,\n",
       "         0.02921456, 0.02647569, 0.02732975, 0.02824074, 0.0297271 ,\n",
       "         0.02689594, 0.02777778, 0.03025794, 0.02824074, 0.02777778,\n",
       "         0.02777778, 0.02647569, 0.02353395, 0.0249183 , 0.0297271 ,\n",
       "         0.0287194 , 0.05648148, 0.02689594, 0.02824074, 0.02777778,\n",
       "         0.02689594, 0.02777778, 0.02455717, 0.02921456, 0.02647569,\n",
       "         0.0297271 , 0.02777778, 0.0249183 , 0.0297271 , 0.02921456,\n",
       "         0.02824074, 0.02921456, 0.02606838])),\n",
       " 'BLEU-1': (0.014178240740736639,\n",
       "  [0.013888888888696002,\n",
       "   0.013888888888696002,\n",
       "   0.013888888888696002,\n",
       "   0.013888888888696002,\n",
       "   0.013888888888696002,\n",
       "   0.013888888888696002,\n",
       "   0.013888888888696002,\n",
       "   0.013888888888696002,\n",
       "   0.013888888888696002,\n",
       "   0.013888888888696002,\n",
       "   0.013888888888696002,\n",
       "   0.013888888888696002,\n",
       "   0.013888888888696002,\n",
       "   0.013888888888696002,\n",
       "   0.013888888888696002,\n",
       "   0.013888888888696002,\n",
       "   0.013888888888696002,\n",
       "   0.013888888888696002,\n",
       "   0.013888888888696002,\n",
       "   0.013888888888696002,\n",
       "   0.013888888888696002,\n",
       "   0.013888888888696002,\n",
       "   0.013888888888696002,\n",
       "   0.013888888888696002,\n",
       "   0.013888888888696002,\n",
       "   0.013888888888696002,\n",
       "   0.013888888888696002,\n",
       "   0.013888888888696002,\n",
       "   0.013888888888696002,\n",
       "   0.013888888888696002,\n",
       "   0.013888888888696002,\n",
       "   0.027777777777391988,\n",
       "   0.013888888888696002,\n",
       "   0.013888888888696002,\n",
       "   0.013888888888696002,\n",
       "   0.013888888888696002,\n",
       "   0.013888888888696002,\n",
       "   0.013888888888696002,\n",
       "   0.013888888888696002,\n",
       "   0.013888888888696002,\n",
       "   0.013888888888696002,\n",
       "   0.013888888888696002,\n",
       "   0.013888888888696002,\n",
       "   0.013888888888696002,\n",
       "   0.013888888888696002,\n",
       "   0.013888888888696002,\n",
       "   0.013888888888696002,\n",
       "   0.013888888888696002]),\n",
       " 'BLEU-2': (6.450024734442439e-11,\n",
       "  [4.422874103557102e-10,\n",
       "   4.422874103557102e-10,\n",
       "   4.422874103557102e-10,\n",
       "   4.422874103557102e-10,\n",
       "   4.422874103557102e-10,\n",
       "   4.422874103557102e-10,\n",
       "   4.422874103557102e-10,\n",
       "   4.422874103557102e-10,\n",
       "   4.422874103557102e-10,\n",
       "   4.422874103557102e-10,\n",
       "   4.422874103557102e-10,\n",
       "   4.422874103557102e-10,\n",
       "   4.422874103557102e-10,\n",
       "   4.422874103557102e-10,\n",
       "   4.422874103557102e-10,\n",
       "   4.422874103557102e-10,\n",
       "   4.422874103557102e-10,\n",
       "   4.422874103557102e-10,\n",
       "   4.422874103557102e-10,\n",
       "   4.422874103557102e-10,\n",
       "   4.422874103557102e-10,\n",
       "   4.422874103557102e-10,\n",
       "   4.422874103557102e-10,\n",
       "   4.422874103557102e-10,\n",
       "   4.422874103557102e-10,\n",
       "   4.422874103557102e-10,\n",
       "   4.422874103557102e-10,\n",
       "   4.422874103557102e-10,\n",
       "   4.422874103557102e-10,\n",
       "   4.422874103557102e-10,\n",
       "   4.422874103557102e-10,\n",
       "   6.254888541919197e-10,\n",
       "   4.422874103557102e-10,\n",
       "   4.422874103557102e-10,\n",
       "   4.422874103557102e-10,\n",
       "   4.422874103557102e-10,\n",
       "   4.422874103557102e-10,\n",
       "   4.422874103557102e-10,\n",
       "   4.422874103557102e-10,\n",
       "   4.422874103557102e-10,\n",
       "   4.422874103557102e-10,\n",
       "   4.422874103557102e-10,\n",
       "   4.422874103557102e-10,\n",
       "   4.422874103557102e-10,\n",
       "   4.422874103557102e-10,\n",
       "   4.422874103557102e-10,\n",
       "   4.422874103557102e-10,\n",
       "   4.422874103557102e-10]),\n",
       " 'BLEU-3': (1.0738109482366869e-13,\n",
       "  [1.4085438495469813e-12,\n",
       "   1.4085438495469813e-12,\n",
       "   1.4085438495469813e-12,\n",
       "   1.4085438495469813e-12,\n",
       "   1.4085438495469813e-12,\n",
       "   1.4085438495469813e-12,\n",
       "   1.4085438495469813e-12,\n",
       "   1.4085438495469813e-12,\n",
       "   1.4085438495469813e-12,\n",
       "   1.4085438495469813e-12,\n",
       "   1.4085438495469813e-12,\n",
       "   1.4085438495469813e-12,\n",
       "   1.4085438495469813e-12,\n",
       "   1.4085438495469813e-12,\n",
       "   1.4085438495469813e-12,\n",
       "   1.4085438495469813e-12,\n",
       "   1.4085438495469813e-12,\n",
       "   1.4085438495469813e-12,\n",
       "   1.4085438495469813e-12,\n",
       "   1.4085438495469813e-12,\n",
       "   1.4085438495469813e-12,\n",
       "   1.4085438495469813e-12,\n",
       "   1.4085438495469813e-12,\n",
       "   1.4085438495469813e-12,\n",
       "   1.4085438495469813e-12,\n",
       "   1.4085438495469813e-12,\n",
       "   1.4085438495469813e-12,\n",
       "   1.4085438495469813e-12,\n",
       "   1.4085438495469813e-12,\n",
       "   1.4085438495469813e-12,\n",
       "   1.4085438495469813e-12,\n",
       "   1.7746540457441984e-12,\n",
       "   1.4085438495469813e-12,\n",
       "   1.4085438495469813e-12,\n",
       "   1.4085438495469813e-12,\n",
       "   1.4085438495469813e-12,\n",
       "   1.4085438495469813e-12,\n",
       "   1.4085438495469813e-12,\n",
       "   1.4085438495469813e-12,\n",
       "   1.4085438495469813e-12,\n",
       "   1.4085438495469813e-12,\n",
       "   1.4085438495469813e-12,\n",
       "   1.4085438495469813e-12,\n",
       "   1.4085438495469813e-12,\n",
       "   1.4085438495469813e-12,\n",
       "   1.4085438495469813e-12,\n",
       "   1.4085438495469813e-12,\n",
       "   1.4085438495469813e-12]),\n",
       " 'BLEU-4': (4.397170327568379e-15,\n",
       "  [7.977475994726905e-14,\n",
       "   7.977475994726905e-14,\n",
       "   7.977475994726905e-14,\n",
       "   7.977475994726905e-14,\n",
       "   7.977475994726905e-14,\n",
       "   7.977475994726905e-14,\n",
       "   7.977475994726905e-14,\n",
       "   7.977475994726905e-14,\n",
       "   7.977475994726905e-14,\n",
       "   7.977475994726905e-14,\n",
       "   7.977475994726905e-14,\n",
       "   7.977475994726905e-14,\n",
       "   7.977475994726905e-14,\n",
       "   7.977475994726905e-14,\n",
       "   7.977475994726905e-14,\n",
       "   7.977475994726905e-14,\n",
       "   7.977475994726905e-14,\n",
       "   7.977475994726905e-14,\n",
       "   7.977475994726905e-14,\n",
       "   7.977475994726905e-14,\n",
       "   7.977475994726905e-14,\n",
       "   7.977475994726905e-14,\n",
       "   7.977475994726905e-14,\n",
       "   7.977475994726905e-14,\n",
       "   7.977475994726905e-14,\n",
       "   7.977475994726905e-14,\n",
       "   7.977475994726905e-14,\n",
       "   7.977475994726905e-14,\n",
       "   7.977475994726905e-14,\n",
       "   7.977475994726905e-14,\n",
       "   7.977475994726905e-14,\n",
       "   9.486871212692643e-14,\n",
       "   7.977475994726905e-14,\n",
       "   7.977475994726905e-14,\n",
       "   7.977475994726905e-14,\n",
       "   7.977475994726905e-14,\n",
       "   7.977475994726905e-14,\n",
       "   7.977475994726905e-14,\n",
       "   7.977475994726905e-14,\n",
       "   7.977475994726905e-14,\n",
       "   7.977475994726905e-14,\n",
       "   7.977475994726905e-14,\n",
       "   7.977475994726905e-14,\n",
       "   7.977475994726905e-14,\n",
       "   7.977475994726905e-14,\n",
       "   7.977475994726905e-14,\n",
       "   7.977475994726905e-14,\n",
       "   7.977475994726905e-14])}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_results(output_rephrases_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<START> room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room',\n",
       " '<START> room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room',\n",
       " '<START> room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room',\n",
       " '<START> room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room',\n",
       " '<START> room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room',\n",
       " '<START> room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room',\n",
       " '<START> room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room',\n",
       " '<START> room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room',\n",
       " '<START> room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room',\n",
       " '<START> room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room',\n",
       " '<START> room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room',\n",
       " '<START> room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room',\n",
       " '<START> room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room',\n",
       " '<START> room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room',\n",
       " '<START> room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room',\n",
       " '<START> room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room',\n",
       " '<START> room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room',\n",
       " '<START> room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room',\n",
       " '<START> room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room',\n",
       " '<START> room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room',\n",
       " '<START> room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room',\n",
       " '<START> room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room',\n",
       " '<START> room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room',\n",
       " '<START> room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room',\n",
       " '<START> room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room',\n",
       " '<START> room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room',\n",
       " '<START> room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room',\n",
       " '<START> room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room',\n",
       " '<START> room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room',\n",
       " '<START> room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room',\n",
       " '<START> room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room',\n",
       " '<START> room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room',\n",
       " '<START> room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room',\n",
       " '<START> room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room',\n",
       " '<START> room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room',\n",
       " '<START> room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room',\n",
       " '<START> room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room',\n",
       " '<START> room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room',\n",
       " '<START> room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room',\n",
       " '<START> room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room',\n",
       " '<START> room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room',\n",
       " '<START> room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room',\n",
       " '<START> room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room',\n",
       " '<START> room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room',\n",
       " '<START> room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room',\n",
       " '<START> room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room',\n",
       " '<START> room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room',\n",
       " '<START> room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room room']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert(output_rephrases_glove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are bad. I had slightly more varied answers with 40 epochs.\n",
    "The validation loss constantly increases for any epoch count."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7e80fdab2d3ad9efed371898a93eb8b1ed0f8415180d1e274f951213442b4295"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('py38_ml': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
